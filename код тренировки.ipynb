{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aq9NQBOSyK7F",
    "outputId": "03810a59-b5d0-499d-fe4f-6b6b634f1775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /mnt/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/mnt/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-09T06:45:32.159296Z",
     "iopub.status.busy": "2025-07-09T06:45:32.158989Z",
     "iopub.status.idle": "2025-07-09T06:45:37.960007Z",
     "shell.execute_reply": "2025-07-09T06:45:37.959068Z",
     "shell.execute_reply.started": "2025-07-09T06:45:32.159239Z"
    },
    "id": "80vDlHi_dlIh",
    "outputId": "57a1d906-9d28-4dcf-d828-388fafff5bf2",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.6.15)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gdown\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "execution": {
     "iopub.execute_input": "2025-07-09T06:45:40.826963Z",
     "iopub.status.busy": "2025-07-09T06:45:40.826659Z",
     "iopub.status.idle": "2025-07-09T06:45:43.466890Z",
     "shell.execute_reply": "2025-07-09T06:45:43.466153Z",
     "shell.execute_reply.started": "2025-07-09T06:45:40.826929Z"
    },
    "id": "5Xu-xLl2dl2G",
    "outputId": "2d489fbc-c18b-4058-b5ed-128ca9f3b3b0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1zbD7J9dkiBz3E5eZ4hbydb559jDP7bvN\n",
      "From (redirected): https://drive.google.com/uc?id=1zbD7J9dkiBz3E5eZ4hbydb559jDP7bvN&confirm=t&uuid=d5ce1687-d919-4fa1-9d6e-48b16b8302f3\n",
      "To: /kaggle/working/posformer_best_v4.pth\n",
      "100%|██████████| 89.0M/89.0M [00:00<00:00, 101MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'posformer_best_v4.pth'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdown.download(f\"https://drive.google.com/uc?id=1zbD7J9dkiBz3E5eZ4hbydb559jDP7bvN\", output=\"posformer_best_v4.pth\", quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2N0Jp2Z7xmde",
    "outputId": "41977bf1-92d5-4e56-e9f8-0ddfd792091e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/namphanvanhai/crohme?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25.4M/25.4M [00:00<00:00, 113MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source import complete.\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "namphanvanhai_crohme_path = kagglehub.dataset_download('namphanvanhai/crohme')\n",
    "\n",
    "print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-05T06:44:38.713377Z",
     "iopub.status.busy": "2025-07-05T06:44:38.712766Z",
     "iopub.status.idle": "2025-07-05T06:44:38.724974Z",
     "shell.execute_reply": "2025-07-05T06:44:38.724126Z",
     "shell.execute_reply.started": "2025-07-05T06:44:38.713351Z"
    },
    "id": "4W14tnAN3ieL",
    "outputId": "9fd06461-b7ef-47fe-c908-b51648aa6262",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['29_em_153.bmp', '514_em_347.bmp', 'RIT_2014_215.bmp', '512_em_299.bmp', 'RIT_2014_10.bmp', '501_em_21.bmp', 'RIT_2014_39.bmp', 'RIT_2014_199.bmp', '501_em_3.bmp', '34_em_249.bmp', '27_em_118.bmp', '513_em_317.bmp', '506_em_59.bmp', '515_em_372.bmp', '516_em_398.bmp', '516_em_385.bmp', '519_em_440.bmp', 'RIT_2014_20.bmp', '504_em_45.bmp', '36_em_29.bmp', '37_em_31.bmp', '503_em_32.bmp', '506_em_58.bmp', '515_em_351.bmp', 'RIT_2014_229.bmp', '511_em_256.bmp', '501_em_19.bmp', '20_em_43.bmp', 'RIT_2014_161.bmp', '31_em_194.bmp', '29_em_152.bmp', 'RIT_2014_3.bmp', 'RIT_2014_23.bmp', '501_em_17.bmp', '518_em_421.bmp', '502_em_1.bmp', 'RIT_2014_64.bmp', '507_em_75.bmp', '503_em_30.bmp', '31_em_198.bmp', 'RIT_2014_239.bmp', 'RIT_2014_81.bmp', 'RIT_2014_6.bmp', '27_em_102.bmp', '26_em_96.bmp', '32_em_220b.bmp', '26_em_91.bmp', 'RIT_2014_175.bmp', 'RIT_2014_43.bmp', 'RIT_2014_225.bmp', '26_em_78.bmp', '516_em_383.bmp', '36_em_45.bmp', '28_em_125.bmp', '511_em_274.bmp', 'RIT_2014_298.bmp', '514_em_337.bmp', '18_em_17.bmp', '513_em_316.bmp', '508_em_86.bmp', '516_em_386.bmp', 'RIT_2014_309.bmp', '515_em_360.bmp', '510_em_100.bmp', 'RIT_2014_190.bmp', '513_em_306.bmp', '26_em_87.bmp', 'RIT_2014_63.bmp', 'RIT_2014_303.bmp', '35_em_22.bmp', 'RIT_2014_129.bmp', '37_em_22.bmp', '508_em_82.bmp', 'RIT_2014_162.bmp', '518_em_418.bmp', '502_em_10.bmp', 'RIT_2014_137.bmp', '37_em_25.bmp', '500_em_110.bmp', '501_em_6.bmp', '518_em_417.bmp', 'RIT_2014_242.bmp', '28_em_141.bmp', 'RIT_2014_59.bmp', 'RIT_2014_19.bmp', '28_em_146.bmp', 'RIT_2014_103.bmp', '26_em_85.bmp', '513_em_319.bmp', '519_em_439.bmp', 'RIT_2014_157.bmp', 'RIT_2014_196.bmp', '18_em_12.bmp', '34_em_235.bmp', '515_em_370.bmp', 'RIT_2014_297.bmp', '513_em_324.bmp', 'RIT_2014_101.bmp', 'RIT_2014_288.bmp', '502_em_22.bmp', 'RIT_2014_241.bmp', '29_em_174.bmp', '32_em_204.bmp', '23_em_52.bmp', '29_em_161.bmp', '507_em_72.bmp', '502_em_13.bmp', '26_em_98.bmp', 'RIT_2014_91.bmp', '516_em_390.bmp', '27_em_117.bmp', 'RIT_2014_296.bmp', 'RIT_2014_51.bmp', '505_em_52.bmp', '503_em_25.bmp', '31_em_180.bmp', '27_em_105.bmp', '501_em_12.bmp', '37_em_3.bmp', 'RIT_2014_171.bmp', '26_em_86.bmp', '505_em_54.bmp', '35_em_8.bmp', '36_em_27.bmp', '26_em_77.bmp', '34_em_248.bmp', '512_em_288.bmp', 'RIT_2014_26.bmp', 'RIT_2014_113.bmp', '23_em_62.bmp', '18_em_13.bmp', '518_em_432.bmp', 'RIT_2014_202.bmp', 'RIT_2014_163.bmp', '31_em_185.bmp', 'RIT_2014_96.bmp', '512_em_293.bmp', 'RIT_2014_106.bmp', '516_em_388.bmp', 'RIT_2014_41.bmp', '29_em_166.bmp', '18_em_20.bmp', 'RIT_2014_292.bmp', 'RIT_2014_11.bmp', 'RIT_2014_276.bmp', 'RIT_2014_29.bmp', '32_em_216.bmp', 'RIT_2014_258.bmp', '36_em_46.bmp', '506_em_66.bmp', 'RIT_2014_238.bmp', 'RIT_2014_247.bmp', '513_em_322.bmp', 'RIT_2014_257.bmp', '519_em_447.bmp', '37_em_30.bmp', 'RIT_2014_32.bmp', 'RIT_2014_68.bmp', 'RIT_2014_201.bmp', '34_em_245.bmp', '504_em_35.bmp', '509_em_89.bmp', 'RIT_2014_165.bmp', '32_em_209.bmp', '20_em_25.bmp', '32_em_210.bmp', '31_em_189.bmp', 'RIT_2014_111.bmp', 'RIT_2014_216.bmp', '32_em_218.bmp', '507_em_76.bmp', '18_em_24.bmp', '35_em_6.bmp', 'RIT_2014_5.bmp', '35_em_4.bmp', '29_em_170.bmp', '515_em_350.bmp', 'RIT_2014_212.bmp', '518_em_423.bmp', 'RIT_2014_25.bmp', '37_em_17.bmp', '507_em_70.bmp', '35_em_18.bmp', '517_em_400.bmp', 'RIT_2014_28.bmp', 'RIT_2014_256.bmp', 'RIT_2014_114.bmp', '27_em_114.bmp', '36_em_38.bmp', 'RIT_2014_177.bmp', 'RIT_2014_207.bmp', '23_em_73.bmp', '27_em_115.bmp', '18_em_11.bmp', '518_em_431.bmp', '34_em_239.bmp', '28_em_133.bmp', '29_em_164.bmp', '27_em_112.bmp', '20_em_48.bmp', '32_em_217.bmp', '515_em_363.bmp', 'RIT_2014_121.bmp', '29_em_165.bmp', '501_em_9.bmp', 'RIT_2014_90.bmp', '501_em_1.bmp', '28_em_149.bmp', 'RIT_2014_87.bmp', '516_em_384.bmp', 'RIT_2014_204.bmp', 'RIT_2014_259.bmp', '20_em_33.bmp', '514_em_329.bmp', 'RIT_2014_221.bmp', '18_em_3.bmp', '501_em_13.bmp', '23_em_55.bmp', 'RIT_2014_134.bmp', '504_em_37.bmp', '31_em_175.bmp', '519_em_462.bmp', '517_em_404.bmp', 'RIT_2014_105.bmp', '29_em_167.bmp', '28_em_147.bmp', 'RIT_2014_7.bmp', '503_em_27.bmp', '516_em_382.bmp', 'RIT_2014_224.bmp', 'RIT_2014_50.bmp', '517_em_408.bmp', '509_em_96.bmp', 'RIT_2014_120.bmp', 'RIT_2014_141.bmp', 'RIT_2014_269.bmp', '507_em_68.bmp', 'RIT_2014_1.bmp', 'RIT_2014_170.bmp', '26_em_89.bmp', '23_em_53.bmp', '28_em_139.bmp', '34_em_230.bmp', 'RIT_2014_244.bmp', '23_em_51.bmp', 'RIT_2014_117.bmp', '514_em_338.bmp', '31_em_196.bmp', 'RIT_2014_38.bmp', '501_em_10.bmp', '506_em_64.bmp', '18_em_9.bmp', '520_em_466.bmp', 'RIT_2014_265.bmp', 'RIT_2014_40.bmp', '507_em_74.bmp', 'RIT_2014_291.bmp', '513_em_300.bmp', '18_em_15.bmp', '35_em_7.bmp', '32_em_219.bmp', 'RIT_2014_144.bmp', '35_em_23.bmp', '35_em_16.bmp', '518_em_435.bmp', '516_em_394.bmp', 'RIT_2014_102.bmp', '518_em_414.bmp', '36_em_41.bmp', '18_em_7.bmp', '512_em_285.bmp', '505_em_49.bmp', '519_em_443.bmp', '20_em_32.bmp', '36_em_33.bmp', '27_em_100.bmp', '29_em_160.bmp', '36_em_49.bmp', '509_em_93.bmp', '29_em_159.bmp', '27_em_122.bmp', '517_em_413.bmp', '509_em_91.bmp', '34_em_231.bmp', '513_em_301.bmp', 'RIT_2014_73.bmp', '515_em_354.bmp', '20_em_28.bmp', 'RIT_2014_188.bmp', '20_em_38.bmp', 'RIT_2014_99.bmp', 'RIT_2014_82.bmp', '29_em_172.bmp', '510_em_106.bmp', 'RIT_2014_295.bmp', 'RIT_2014_70.bmp', '26_em_80.bmp', '515_em_369.bmp', 'RIT_2014_240.bmp', '23_em_57.bmp', '18_em_8.bmp', '518_em_438.bmp', 'RIT_2014_233.bmp', '31_em_193.bmp', '516_em_396.bmp', '32_em_202.bmp', '504_em_39.bmp', 'RIT_2014_124.bmp', '32_em_214.bmp', '513_em_302.bmp', 'RIT_2014_15.bmp', 'RIT_2014_127.bmp', '512_em_298.bmp', '35_em_17.bmp', '502_em_11.bmp', 'RIT_2014_138.bmp', '519_em_442.bmp', '511_em_252.bmp', '502_em_12.bmp', '515_em_373.bmp', 'RIT_2014_285.bmp', '34_em_246.bmp', 'RIT_2014_280.bmp', '35_em_20.bmp', '20_em_45.bmp', '502_em_16.bmp', 'RIT_2014_251.bmp', 'RIT_2014_218.bmp', '515_em_353.bmp', '26_em_79.bmp', '513_em_311.bmp', '511_em_259.bmp', '518_em_437.bmp', '27_em_104.bmp', '519_em_463.bmp', '37_em_23.bmp', 'RIT_2014_211.bmp', '37_em_28.bmp', 'RIT_2014_245.bmp', 'RIT_2014_310.bmp', '509_em_97.bmp', '502_em_8.bmp', '32_em_206.bmp', '501_em_22.bmp', '28_em_136.bmp', '35_em_19.bmp', '501_em_14.bmp', '505_em_56.bmp', '516_em_397.bmp', '23_em_61.bmp', '519_em_459.bmp', 'RIT_2014_185.bmp', '29_em_156.bmp', '514_em_334.bmp', '20_em_41.bmp', '508_em_80.bmp', 'RIT_2014_195.bmp', '37_em_15.bmp', '36_em_25.bmp', '37_em_20.bmp', '35_em_0.bmp', '516_em_392.bmp', 'RIT_2014_154.bmp', '516_em_387.bmp', '34_em_234.bmp', '36_em_36.bmp', 'RIT_2014_88.bmp', '519_em_445.bmp', '516_em_379.bmp', '519_em_457.bmp', '28_em_137.bmp', '28_em_131.bmp', 'RIT_2014_46.bmp', '515_em_366.bmp', '32_em_223.bmp', 'RIT_2014_301.bmp', 'RIT_2014_253.bmp', 'RIT_2014_65.bmp', 'RIT_2014_232.bmp', '28_em_145.bmp', 'RIT_2014_219.bmp', '513_em_303.bmp', '514_em_335.bmp', '501_em_8.bmp', '512_em_277.bmp', '514_em_333.bmp', '31_em_182.bmp', 'RIT_2014_118.bmp', '20_em_46.bmp', 'RIT_2014_79.bmp', 'RIT_2014_147.bmp', 'RIT_2014_119.bmp', '32_em_220c.bmp', '517_em_409.bmp', 'RIT_2014_94.bmp', 'RIT_2014_250.bmp', 'RIT_2014_205.bmp', 'RIT_2014_135.bmp', '34_em_244.bmp', '508_em_83.bmp', '508_em_79.bmp', '514_em_346.bmp', 'RIT_2014_72.bmp', '35_em_15.bmp', '515_em_362.bmp', 'RIT_2014_231.bmp', '514_em_332.bmp', '23_em_63.bmp', '511_em_260.bmp', 'RIT_2014_92.bmp', 'RIT_2014_287.bmp', '509_em_99.bmp', '28_em_140.bmp', 'RIT_2014_145.bmp', '27_em_101.bmp', 'RIT_2014_266.bmp', '504_em_36.bmp', '35_em_3.bmp', 'RIT_2014_312.bmp', 'RIT_2014_22.bmp', '18_em_4.bmp', '31_em_197.bmp', '36_em_37.bmp', 'RIT_2014_255.bmp', '513_em_318.bmp', '28_em_126.bmp', '27_em_109.bmp', '37_em_32.bmp', '23_em_59.bmp', 'RIT_2014_186.bmp', '505_em_48.bmp', '29_em_151.bmp', '515_em_361.bmp', '26_em_76.bmp', 'RIT_2014_131.bmp', '37_em_4.bmp', 'RIT_2014_283.bmp', '509_em_92.bmp', '511_em_254.bmp', '504_em_38.bmp', 'RIT_2014_128.bmp', '26_em_95.bmp', '34_em_242.bmp', 'RIT_2014_140.bmp', 'RIT_2014_77.bmp', '512_em_291.bmp', 'RIT_2014_158.bmp', '518_em_434.bmp', '20_em_27.bmp', '515_em_368.bmp', '27_em_116.bmp', '514_em_326.bmp', '32_em_208.bmp', '517_em_407.bmp', 'RIT_2014_122.bmp', '508_em_88.bmp', '32_em_221.bmp', 'RIT_2014_149.bmp', '504_em_43.bmp', '518_em_433.bmp', '514_em_325.bmp', '500_em_109.bmp', '26_em_83.bmp', '18_em_10.bmp', '31_em_183.bmp', 'RIT_2014_284.bmp', '18_em_19.bmp', '23_em_54.bmp', '511_em_250.bmp', '503_em_28.bmp', '508_em_78.bmp', '511_em_264.bmp', 'RIT_2014_282.bmp', '35_em_21.bmp', '507_em_73.bmp', '36_em_39.bmp', '519_em_458.bmp', '32_em_220a.bmp', 'RIT_2014_213.bmp', 'RIT_2014_299.bmp', '23_em_70.bmp', '511_em_268.bmp', '26_em_84.bmp', '20_em_34.bmp', '34_em_229.bmp', '23_em_69.bmp', 'RIT_2014_179.bmp', '511_em_257.bmp', '20_em_30.bmp', 'RIT_2014_279.bmp', '514_em_328.bmp', 'RIT_2014_85.bmp', 'RIT_2014_123.bmp', 'RIT_2014_74.bmp', 'RIT_2014_30.bmp', '514_em_330.bmp', '31_em_177.bmp', '511_em_253.bmp', '23_em_67.bmp', '20_em_40.bmp', 'RIT_2014_71.bmp', 'RIT_2014_184.bmp', '512_em_290.bmp', '519_em_451.bmp', 'RIT_2014_248.bmp', '31_em_199.bmp', 'RIT_2014_48.bmp', '515_em_358.bmp', '20_em_31.bmp', 'RIT_2014_164.bmp', '519_em_444.bmp', 'RIT_2014_214.bmp', '36_em_26.bmp', 'RIT_2014_60.bmp', '37_em_9.bmp', '518_em_430.bmp', '28_em_138.bmp', 'RIT_2014_183.bmp', '509_em_94.bmp', 'RIT_2014_181.bmp', '28_em_142.bmp', '37_em_14.bmp', 'RIT_2014_89.bmp', '35_em_24.bmp', 'RIT_2014_97.bmp', 'RIT_2014_180.bmp', '23_em_50.bmp', '512_em_275.bmp', 'RIT_2014_45.bmp', 'RIT_2014_107.bmp', '512_em_286.bmp', '514_em_339.bmp', 'RIT_2014_150.bmp', '27_em_123.bmp', '518_em_422.bmp', 'RIT_2014_133.bmp', 'RIT_2014_200.bmp', '34_em_243.bmp', '517_em_406.bmp', '513_em_310.bmp', '518_em_420.bmp', '37_em_13.bmp', '32_em_205.bmp', '510_em_107.bmp', '504_em_46.bmp', 'RIT_2014_271.bmp', '37_em_24.bmp', '519_em_441.bmp', 'RIT_2014_174.bmp', '32_em_211.bmp', '32_em_215.bmp', '31_em_176.bmp', '500_em_108.bmp', '37_em_6.bmp', 'RIT_2014_268.bmp', '520_em_467.bmp', 'RIT_2014_62.bmp', '517_em_401.bmp', 'RIT_2014_44.bmp', '518_em_427.bmp', 'RIT_2014_78.bmp', '502_em_9.bmp', 'RIT_2014_227.bmp', '27_em_110.bmp', '36_em_28.bmp', '513_em_314.bmp', '20_em_29.bmp', '502_em_14.bmp', '502_em_17.bmp', '27_em_106.bmp', '29_em_163.bmp', '36_em_44.bmp', '29_em_162.bmp', 'RIT_2014_2.bmp', 'RIT_2014_4.bmp', '501_em_23.bmp', '515_em_364.bmp', '513_em_312.bmp', '511_em_273.bmp', '37_em_10.bmp', '516_em_376.bmp', 'RIT_2014_243.bmp', '34_em_240.bmp', 'RIT_2014_17.bmp', '516_em_395.bmp', '32_em_213.bmp', 'RIT_2014_115.bmp', '514_em_327.bmp', 'RIT_2014_12.bmp', '511_em_270.bmp', 'RIT_2014_264.bmp', 'RIT_2014_278.bmp', 'RIT_2014_37.bmp', '37_em_27.bmp', '31_em_191.bmp', 'RIT_2014_182.bmp', 'RIT_2014_252.bmp', 'RIT_2014_61.bmp', '37_em_18.bmp', '517_em_405.bmp', '32_em_201.bmp', '20_em_26.bmp', 'RIT_2014_249.bmp', '506_em_60.bmp', 'RIT_2014_66.bmp', '28_em_130.bmp', '514_em_348.bmp', 'RIT_2014_84.bmp', '28_em_128.bmp', 'RIT_2014_112.bmp', '504_em_41.bmp', 'RIT_2014_237.bmp', '28_em_135.bmp', '23_em_72.bmp', 'RIT_2014_155.bmp', '518_em_436.bmp', 'RIT_2014_8.bmp', '32_em_200.bmp', '36_em_34.bmp', '31_em_188.bmp', 'RIT_2014_98.bmp', 'RIT_2014_146.bmp', '517_em_410.bmp', 'RIT_2014_80.bmp', '505_em_50.bmp', '37_em_7.bmp', '514_em_343.bmp', '26_em_94.bmp', '26_em_82.bmp', 'RIT_2014_189.bmp', '518_em_426.bmp', '28_em_144.bmp', '513_em_323.bmp', 'RIT_2014_304.bmp', '501_em_4.bmp', 'RIT_2014_42.bmp', '26_em_81.bmp', 'RIT_2014_9.bmp', '502_em_2.bmp', '26_em_90.bmp', '513_em_320.bmp', '36_em_42.bmp', '31_em_184.bmp', '515_em_367.bmp', '37_em_26.bmp', '513_em_304.bmp', '18_em_18.bmp', '29_em_169.bmp', '518_em_419.bmp', '509_em_98.bmp', '37_em_2.bmp', 'RIT_2014_95.bmp', '514_em_331.bmp', '28_em_129.bmp', '34_em_226.bmp', '505_em_53.bmp', 'RIT_2014_143.bmp', 'RIT_2014_24.bmp', '35_em_5.bmp', '512_em_295.bmp', 'RIT_2014_246.bmp', '28_em_143.bmp', '18_em_23.bmp', '34_em_228.bmp', '26_em_99.bmp', 'RIT_2014_203.bmp', '515_em_374.bmp', 'RIT_2014_136.bmp', 'RIT_2014_198.bmp', '518_em_425.bmp', '508_em_81.bmp', '510_em_105.bmp', '37_em_5.bmp', '32_em_224.bmp', '502_em_0.bmp', 'RIT_2014_302.bmp', '18_em_16.bmp', '29_em_155.bmp', '31_em_195.bmp', '20_em_49.bmp', '509_em_95.bmp', 'RIT_2014_36.bmp', '510_em_104.bmp', 'RIT_2014_156.bmp', '501_em_5.bmp', '506_em_65.bmp', 'RIT_2014_273.bmp', '23_em_64.bmp', 'RIT_2014_206.bmp', 'RIT_2014_223.bmp', '18_em_5.bmp', '507_em_69.bmp', '511_em_267.bmp', '34_em_238.bmp', '37_em_12.bmp', 'RIT_2014_263.bmp', '31_em_178.bmp', 'RIT_2014_126.bmp', '26_em_75.bmp', '36_em_32.bmp', '35_em_14.bmp', 'RIT_2014_192.bmp', '512_em_279.bmp', 'RIT_2014_67.bmp', '18_em_14.bmp', 'RIT_2014_152.bmp', 'RIT_2014_294.bmp', '505_em_47.bmp', '28_em_134.bmp', 'RIT_2014_130.bmp', '501_em_7.bmp', '512_em_282.bmp', '511_em_272.bmp', '502_em_24.bmp', 'RIT_2014_173.bmp', '28_em_148.bmp', 'RIT_2014_58.bmp', '32_em_207.bmp', '20_em_39.bmp', '512_em_284.bmp', '519_em_460.bmp', 'RIT_2014_217.bmp', 'RIT_2014_35.bmp', '20_em_42.bmp', '512_em_292.bmp', '514_em_342.bmp', '512_em_280.bmp', '512_em_294.bmp', '37_em_11.bmp', 'RIT_2014_21.bmp', '511_em_255.bmp', '506_em_62.bmp', '520_em_465.bmp', '513_em_307.bmp', '502_em_4.bmp', '511_em_251.bmp', '501_em_0.bmp', '510_em_103.bmp', 'RIT_2014_49.bmp', 'RIT_2014_109.bmp', '32_em_203.bmp', '511_em_271.bmp', '27_em_121.bmp', '502_em_21.bmp', 'RIT_2014_176.bmp', 'RIT_2014_178.bmp', 'RIT_2014_290.bmp', '517_em_402.bmp', '515_em_357.bmp', '34_em_236.bmp', 'RIT_2014_27.bmp', '502_em_3.bmp', '501_em_18.bmp', '36_em_47.bmp', '502_em_18.bmp', 'RIT_2014_132.bmp', '23_em_60.bmp', '37_em_8.bmp', '513_em_308.bmp', 'RIT_2014_311.bmp', 'RIT_2014_110.bmp', '515_em_365.bmp', '508_em_87.bmp', 'RIT_2014_100.bmp', '510_em_101.bmp', '503_em_31.bmp', 'RIT_2014_93.bmp', '36_em_30.bmp', '27_em_108.bmp', '506_em_67.bmp', '34_em_232.bmp', '28_em_132.bmp', '518_em_429.bmp', '503_em_26.bmp', '514_em_336.bmp', 'RIT_2014_169.bmp', 'RIT_2014_262.bmp', '511_em_269.bmp', '504_em_40.bmp', '519_em_450.bmp', 'RIT_2014_281.bmp', 'RIT_2014_31.bmp', '515_em_371.bmp', 'RIT_2014_293.bmp', '27_em_113.bmp', '31_em_181.bmp', 'RIT_2014_300.bmp', '34_em_225.bmp', '506_em_61.bmp', '37_em_21.bmp', 'RIT_2014_69.bmp', 'RIT_2014_308.bmp', '28_em_127.bmp', 'RIT_2014_230.bmp', '503_em_33.bmp', '20_em_36.bmp', 'RIT_2014_234.bmp', '23_em_58.bmp', 'RIT_2014_260.bmp', '512_em_296.bmp', '516_em_399.bmp', 'RIT_2014_86.bmp', '512_em_276.bmp', '517_em_412.bmp', '512_em_297.bmp', '37_em_1.bmp', 'RIT_2014_151.bmp', '29_em_173.bmp', '518_em_424.bmp', '23_em_71.bmp', 'RIT_2014_168.bmp', '514_em_345.bmp', '513_em_313.bmp', '23_em_68.bmp', '37_em_29.bmp', '515_em_359.bmp', '20_em_35.bmp', '35_em_13.bmp', 'RIT_2014_125.bmp', '501_em_15.bmp', '512_em_278.bmp', '518_em_428.bmp', '511_em_258.bmp', 'RIT_2014_236.bmp', 'RIT_2014_222.bmp', '20_em_37.bmp', 'RIT_2014_267.bmp', '501_em_11.bmp', '501_em_20.bmp', 'RIT_2014_83.bmp', 'RIT_2014_56.bmp', '29_em_171.bmp', 'RIT_2014_153.bmp', '36_em_35.bmp', 'RIT_2014_226.bmp', '26_em_97.bmp', '504_em_42.bmp', '513_em_305.bmp', 'RIT_2014_193.bmp', '26_em_88.bmp', '35_em_2.bmp', '36_em_48.bmp', 'RIT_2014_108.bmp', '37_em_19.bmp', '515_em_356.bmp', '18_em_0.bmp', '514_em_340.bmp', '519_em_448.bmp', '517_em_403.bmp', '35_em_1.bmp', '34_em_241.bmp', 'RIT_2014_305.bmp', 'RIT_2014_228.bmp', '502_em_7.bmp', 'RIT_2014_261.bmp', '32_em_222.bmp', '517_em_411.bmp', 'RIT_2014_289.bmp', '34_em_237.bmp', '512_em_283.bmp', '31_em_187.bmp', '516_em_377.bmp', '519_em_461.bmp', 'RIT_2014_172.bmp', '516_em_393.bmp', '510_em_102.bmp', '27_em_103.bmp', '27_em_119.bmp', '506_em_63.bmp', 'RIT_2014_275.bmp', 'RIT_2014_274.bmp', 'RIT_2014_53.bmp', '20_em_44.bmp', '27_em_111.bmp', 'RIT_2014_14.bmp', '512_em_289.bmp', '511_em_265.bmp', '519_em_452.bmp', 'RIT_2014_254.bmp', 'RIT_2014_209.bmp', 'RIT_2014_159.bmp', '515_em_352.bmp', '34_em_233.bmp', '37_em_0.bmp', 'RIT_2014_54.bmp', 'RIT_2014_104.bmp', 'RIT_2014_166.bmp', '31_em_192.bmp', 'RIT_2014_208.bmp', '20_em_47.bmp', 'RIT_2014_47.bmp', 'RIT_2014_191.bmp', '511_em_262.bmp', '18_em_1.bmp', '502_em_23.bmp', '516_em_389.bmp', 'RIT_2014_57.bmp', '512_em_281.bmp', '35_em_12.bmp', 'RIT_2014_148.bmp', '520_em_464.bmp', 'RIT_2014_18.bmp', '514_em_344.bmp', '513_em_321.bmp', 'RIT_2014_306.bmp', '18_em_6.bmp', '26_em_92.bmp', '35_em_11.bmp', '32_em_212.bmp', '31_em_190.bmp', '29_em_158.bmp', 'RIT_2014_34.bmp', '34_em_227.bmp', '501_em_2.bmp', 'RIT_2014_270.bmp', 'RIT_2014_194.bmp', '26_em_93.bmp', 'RIT_2014_52.bmp', '514_em_341.bmp', '505_em_55.bmp', '513_em_309.bmp', '18_em_21.bmp', '502_em_6.bmp', '501_em_16.bmp', '508_em_84.bmp', '36_em_31.bmp', '27_em_120.bmp', 'RIT_2014_33.bmp', '503_em_29.bmp', '29_em_154.bmp', 'RIT_2014_286.bmp', 'RIT_2014_187.bmp', '518_em_415.bmp', '34_em_247.bmp', 'RIT_2014_13.bmp', '29_em_157.bmp', '23_em_65.bmp', '512_em_287.bmp', '501_em_24.bmp', '509_em_90.bmp', 'RIT_2014_197.bmp', '502_em_15.bmp', 'RIT_2014_75.bmp', '27_em_107.bmp', '504_em_44.bmp', 'RIT_2014_210.bmp', '507_em_77.bmp', 'RIT_2014_220.bmp', 'RIT_2014_16.bmp', '519_em_456.bmp', 'RIT_2014_116.bmp', '516_em_378.bmp', 'RIT_2014_272.bmp', '23_em_56.bmp', 'RIT_2014_76.bmp', '23_em_66.bmp', 'RIT_2014_55.bmp', '505_em_51.bmp', '519_em_454.bmp', '503_em_34.bmp', '508_em_85.bmp', '27_em_124.bmp', '37_em_16.bmp', '502_em_5.bmp', '515_em_355.bmp', '511_em_266.bmp', '18_em_2.bmp', 'RIT_2014_235.bmp', '502_em_19.bmp', '36_em_43.bmp', '518_em_416.bmp', '506_em_57.bmp', 'RIT_2014_277.bmp', 'RIT_2014_142.bmp', '516_em_380.bmp', 'RIT_2014_160.bmp', '36_em_40.bmp', '507_em_71.bmp', '18_em_22.bmp', '502_em_20.bmp', '35_em_10.bmp', 'RIT_2014_167.bmp', '35_em_9.bmp', '29_em_150.bmp', 'RIT_2014_307.bmp', 'RIT_2014_139.bmp']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "print(os.listdir(path.join(namphanvanhai_crohme_path,\"CROHME\",\"2014\",\"img\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T08:12:19.978849Z",
     "iopub.status.busy": "2025-07-07T08:12:19.978320Z",
     "iopub.status.idle": "2025-07-07T08:12:28.859384Z",
     "shell.execute_reply": "2025-07-07T08:12:28.858445Z",
     "shell.execute_reply.started": "2025-07-07T08:12:19.978818Z"
    },
    "id": "Y9y6jajEhZc4",
    "outputId": "043253ba-c585-4f2e-ab4d-90bb319f75d5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trim\n",
      "  Downloading trim-0.3.tar.gz (3.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: trim\n",
      "  Building wheel for trim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for trim: filename=trim-0.3-py3-none-any.whl size=3459 sha256=c1a37ad36731c94a4762f97a7e3a4fd8edc7873653de49b90c446b1765fc14b2\n",
      "  Stored in directory: /root/.cache/pip/wheels/0f/28/33/f5f052e542f8859d0a1b3b897936cb9be800ba768885884b14\n",
      "Successfully built trim\n",
      "Installing collected packages: trim\n",
      "Successfully installed trim-0.3\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install trim\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T11:27:25.402574Z",
     "iopub.status.busy": "2025-07-10T11:27:25.401663Z",
     "iopub.status.idle": "2025-07-10T11:27:25.511849Z",
     "shell.execute_reply": "2025-07-10T11:27:25.511024Z",
     "shell.execute_reply.started": "2025-07-10T11:27:25.402534Z"
    },
    "id": "CrASdHHY8lJL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "#               Импорты и базовые библиотеки\n",
    "# =================================================================================\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from einops import rearrange # Библиотека для удобных операций с тензорами\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm # Библиотека для создания красивых progress bar'ов\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# =================================================================================\n",
    "#                                1. ГИПЕРПАРАМЕТРЫ И КОНСТАНТЫ\n",
    "# =================================================================================\n",
    "# --- Параметры обработки данных ---\n",
    "MAX_SEQ_LEN = 512          # Максимальная длина последовательности токенов LaTeX\n",
    "NUM_NESTED_LEVELS = 10    # Максимальная глубина вложенности структур (a^{b^{c...}})\n",
    "MAX_IDENTIFIER_LEN = 15   # Максимальная длина строки-идентификатора позиции (e.g., 'MRL' -> 3)\n",
    "PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN = '<PAD>', '<SOS>', '<EOS>', '<UNK>' # Специальные токены\n",
    "\n",
    "# --- Параметры архитектуры модели ---\n",
    "D_MODEL = 256             # Основная размерность векторов в модели (embedding, attention)\n",
    "D_FF = 1024               # Размерность в скрытом слое Feed-Forward сетей\n",
    "NUM_HEADS = 8             # Количество голов в Multi-Head Attention\n",
    "GROWTH_RATE = 24          # \"Скорость роста\" в DenseNet, количество новых каналов на каждом слое\n",
    "NUM_ENCODER_LAYERS = 16   # Количество плотных блоков (dense blocks) в энкодере DenseNet\n",
    "NUM_DECODER_LAYERS = 3    # Количество слоев в декодере\n",
    "BOTTLENECK = True         # Использовать ли \"бутылочное горлышко\" (1x1 свертки) в DenseNet для эффективности\n",
    "DROPOUT_RATE = 0.3        # Вероятность dropout для регуляризации модели\n",
    "\n",
    "# =================================================================================\n",
    "#                                2. ТОКЕНИЗАЦИЯ И СЛОВАРИ\n",
    "# =================================================================================\n",
    "\n",
    "# Регулярное выражение для грамотного разбиения строки LaTeX на осмысленные токены.\n",
    "# Оно выделяет:\n",
    "# 1. Команды LaTeX (e.g., \\frac, \\sqrt)\n",
    "# 2. Экранированные скобки и символы\n",
    "# 3. Числа (включая экспоненциальную запись)\n",
    "# 4. Буквенные последовательности\n",
    "# 5. Одиночные математические операторы\n",
    "# 6. Скобки и знаки препинания\n",
    "# 7. Пробелы (которые потом игнорируются)\n",
    "# 8. Любой другой одиночный символ\n",
    "TOKEN_REGEX = re.compile(\n",
    "    r\"(\\\\[a-zA-Z]+(?:\\*)?)|(\\\\\\{|\\\\\\}|\\\\.)|([0-9]+(?:\\.[0-9]+)?(?:[eE][+-]?[0-9]+)?)|([A-Za-z]+)|([+\\-*/=<>!~^_&|%])|([{}()\\[\\],.;:?'])|(\\s+)|(\\S)\"\n",
    ")\n",
    "\n",
    "def tokenize_latex(s: str) -> list[str]:\n",
    "    \"\"\"Разбивает строку LaTeX на список токенов с помощью TOKEN_REGEX.\"\"\"\n",
    "    tokens_grouped = TOKEN_REGEX.findall(s)\n",
    "    tokens = []\n",
    "    for group in tokens_grouped:\n",
    "        # Из каждой найденной группы берем первый непустой элемент\n",
    "        non_empty_token = next(filter(None, group), '')\n",
    "        # Игнорируем пробельные токены\n",
    "        if not non_empty_token.isspace():\n",
    "             tokens.append(non_empty_token)\n",
    "    return tokens\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"\n",
    "    Класс для управления словарем токенов. Отвечает за преобразование\n",
    "    списка токенов в список их числовых ID и обратно.\n",
    "    \"\"\"\n",
    "    def __init__(self, expressions=None, min_freq=1):\n",
    "        # Инициализация со специальными токенами, которые должны быть в начале словаря\n",
    "        self.itos = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
    "        if expressions:\n",
    "            # Подсчет частоты каждого токена по всему датасету\n",
    "            freq = defaultdict(int)\n",
    "            for expr in expressions:\n",
    "                for t in tokenize_latex(expr):\n",
    "                    freq[t] += 1\n",
    "            # Добавляем в словарь только те токены, которые встречаются чаще `min_freq`\n",
    "            self.itos.extend([tok for tok, count in sorted(freq.items(), key=lambda x: -x[1]) if count >= min_freq])\n",
    "        \n",
    "        # Создаем два словаря-отображения: \"токен -> ID\" (stoi) и \"ID -> токен\" (itos)\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
    "        self.pad_id = self.stoi[PAD_TOKEN]\n",
    "        self.sos_id = self.stoi[SOS_TOKEN]\n",
    "        self.eos_id = self.stoi[EOS_TOKEN]\n",
    "        self.unk_id = self.stoi[UNK_TOKEN]\n",
    "        \n",
    "    def encode(self, tokens: list[str]) -> list[int]:\n",
    "        \"\"\"Преобразует список токенов в список их ID.\"\"\"\n",
    "        return [self.stoi.get(t, self.unk_id) for t in tokens]\n",
    "\n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        \"\"\"Преобразует список ID обратно в читаемую строку LaTeX.\"\"\"\n",
    "        # Игнорируем специальные токены при декодировании, чтобы получить чистую строку\n",
    "        tokens = [self.itos[i] for i in ids if i not in {self.pad_id, self.sos_id, self.eos_id}]\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "class PosVocab:\n",
    "    \"\"\"Специальный маленький словарь для кодирования позиционных символов ('M', 'L', 'R').\"\"\"\n",
    "    def __init__(self):\n",
    "        self.itos = ['<PAD>', '<SOS>', '<EOS>', 'M', 'L', 'R']\n",
    "        self.stoi = {s: i for i, s in enumerate(self.itos)}\n",
    "        self.pad_id = self.stoi['<PAD>']\n",
    "        self.sos_id = self.stoi['<SOS>']\n",
    "\n",
    "# =================================================================================\n",
    "#                  3. ПАРСЕР ПОЗИЦИЙ\n",
    "# =================================================================================\n",
    "class PositionForestEncoder:\n",
    "    \"\"\"\n",
    "    Класс, отвечающий за преобразование строки LaTeX в набор позиционных идентификаторов.\n",
    "        \n",
    "    Ее ключевая особенность и фундаментальное отличие от логики в статье PosFormer заключается\n",
    "    в обработке структурных скобок `{}`. Данный парсер присваивает позиционные метки ('L', 'R')\n",
    "    только содержимому (контенту) внутри скобок, но не самим скобкам.\n",
    "    \n",
    "    Пример: для `\\frac{x}{y}` позиционные идентификаторы будут:\n",
    "    ['M', 'M', 'ML', 'M', 'M', 'MR', 'M']\n",
    "    Вместо правильного (по статье):\n",
    "    ['M', 'ML', 'ML', 'ML', 'MR', 'MR', 'MR']\n",
    "    \n",
    "    Это упрощение задачи для модели на ранних этапах, которое позволило быстро дойти до 43%\n",
    "    \"\"\"\n",
    "    def __init__(self, pos_vocab, max_len=MAX_SEQ_LEN, max_identifier_len=MAX_IDENTIFIER_LEN):\n",
    "        self.pos_vocab = pos_vocab\n",
    "        self.max_len = max_len\n",
    "        self.max_identifier_len = max_identifier_len\n",
    "        self._cache = {} # Кэширование для ускорения работы с повторяющимися формулами\n",
    "\n",
    "    def _get_pos_strings(self, tokens):\n",
    "        \"\"\"\n",
    "        Основная логика парсинга. Рекурсивно обходит токены и строит строки позиций.\n",
    "        \"\"\"\n",
    "        cache_key = tuple(tokens)\n",
    "        if cache_key in self._cache:\n",
    "            return self._cache[cache_key]\n",
    "\n",
    "        T = len(tokens)\n",
    "        # Инициализируем все токены базовой позицией 'M' (middle/main)\n",
    "        ids = ['M'] * T\n",
    "\n",
    "        def find_matching_brace(start_index):\n",
    "            \"\"\"Вспомогательная функция для поиска парной закрывающей скобки, учитывая вложенность.\"\"\"\n",
    "            depth = 1\n",
    "            for i in range(start_index + 1, T):\n",
    "                if tokens[i] == '{':\n",
    "                    depth += 1\n",
    "                elif tokens[i] == '}':\n",
    "                    depth -= 1\n",
    "                    if depth == 0:\n",
    "                        return i\n",
    "            return T - 1 # Возвращаем последний индекс, если пара не найдена\n",
    "\n",
    "        def parse_recursive(start, end, current_pos_prefix):\n",
    "            \"\"\"\n",
    "            Рекурсивная функция, обходящая последовательность токенов.\n",
    "            `current_pos_prefix` хранит текущий контекст вложенности (e.g., 'M', 'ML', 'MLR').\n",
    "            \"\"\"\n",
    "            i = start\n",
    "            while i < end:\n",
    "                tok = tokens[i]\n",
    "\n",
    "                # Обработка команд с одним аргументом: ^ (верхний индекс), _ (нижний), \\sqrt (корень)\n",
    "                if tok in ('^', '_', '\\\\sqrt') and i + 1 < end:\n",
    "                    # 'L' (Left/upper) для верхних индексов и корней, 'R' (Right/lower) для нижних\n",
    "                    label = 'L' if tok in ('^', '\\\\sqrt') else 'R'\n",
    "                    \n",
    "                    if tokens[i+1] == '{': # Если аргумент в скобках\n",
    "                        brace_end = find_matching_brace(i + 1)\n",
    "                        \n",
    "                        # !!! КЛЮЧЕВАЯ ОСОБЕННОСТЬ ЭТОГО ПАРСЕРА !!!\n",
    "                        # Цикл идет от i+2 (индекс токена ПОСЛЕ '{') до brace_end (индекс токена ПЕРЕД '}').\n",
    "                        # Таким образом, сами скобки `{` и `}` НЕ ПОЛУЧАЮТ новую позиционную метку.\n",
    "                        # Они остаются с их исходной меткой 'M'.\n",
    "                        for k in range(i + 2, brace_end):\n",
    "                            ids[k] = current_pos_prefix + label\n",
    "                        \n",
    "                        # Рекурсивный вызов для разбора содержимого скобок\n",
    "                        parse_recursive(i + 2, brace_end, current_pos_prefix + label)\n",
    "                        i = brace_end # Перепрыгиваем через весь обработанный блок\n",
    "                    else: # Если аргумент - это один токен без скобок\n",
    "                        ids[i+1] = current_pos_prefix + label\n",
    "                        i += 1 # Пропускаем команду и ее аргумент\n",
    "\n",
    "                # Обработка команды \\frac с двумя аргументами\n",
    "                elif tok == '\\\\frac' and i + 1 < end and tokens[i+1] == '{':\n",
    "                    num_end = find_matching_brace(i + 1) # Находим конец числителя\n",
    "                    if num_end + 1 < end and tokens[num_end + 1] == '{':\n",
    "                        den_end = find_matching_brace(num_end + 1) # Находим конец знаменателя\n",
    "                        \n",
    "                        # --- Числитель (Numerator) ---\n",
    "                        # !!! СНОВА ТА ЖЕ ЛОГИКА: позиционная метка 'L' применяется только к контенту внутри скобок.\n",
    "                        for k in range(i + 2, num_end):\n",
    "                            ids[k] = current_pos_prefix + 'L'\n",
    "                        parse_recursive(i + 2, num_end, current_pos_prefix + 'L')\n",
    "                        \n",
    "                        # --- Знаменатель (Denominator) ---\n",
    "                        # Позиционная метка 'R' также применяется только к контенту.\n",
    "                        for k in range(num_end + 2, den_end):\n",
    "                            ids[k] = current_pos_prefix + 'R'\n",
    "                        parse_recursive(num_end + 2, den_end, current_pos_prefix + 'R')\n",
    "                        \n",
    "                        i = den_end # Перепрыгиваем через всю дробь\n",
    "                    else:\n",
    "                        # Если у \\frac только один аргумент (синтаксически неверно, но обрабатываем), пропускаем его\n",
    "                        i = num_end\n",
    "                \n",
    "                i += 1\n",
    "        \n",
    "        # Запускаем рекурсивный парсинг для всей последовательности токенов\n",
    "        parse_recursive(0, T, 'M')\n",
    "        \n",
    "        # Пост-обработка не требуется, так как ids уже содержит полные строки, но для консистентности\n",
    "        # создается новый список.\n",
    "        final_ids = []\n",
    "        for i in range(T):\n",
    "            if ids[i] == 'M':\n",
    "                final_ids.append('M')\n",
    "            else:\n",
    "                final_ids.append(ids[i])\n",
    "\n",
    "        self._cache[cache_key] = final_ids\n",
    "        return final_ids\n",
    "\n",
    "    def process_formula(self, latex_str: str):\n",
    "        \"\"\"\n",
    "        Главный метод класса. Превращает одну строку LaTeX в словарь с тензорами для обучения.\n",
    "        \"\"\"\n",
    "        # 1. Токенизация и обрезка по максимальной длине\n",
    "        tokens = tokenize_latex(latex_str)\n",
    "        tokens = tokens[:self.max_len - 2]\n",
    "        \n",
    "        # 2. Получение строк позиций от нашего парсера\n",
    "        pos_strings_raw = self._get_pos_strings(tokens)\n",
    "        \n",
    "        # 3. Формирование ground-truth для вспомогательных задач\n",
    "        #    - Уровень вложенности (nested_depth): длина строки 'MLR' -> 3, уровень -> 2\n",
    "        #    - Относительная позиция (rel_pos): последний символ строки ('M', 'L' или 'R')\n",
    "        tokens_gt = [SOS_TOKEN] + tokens + [EOS_TOKEN]\n",
    "        nested_depth_gt = [0] + [min(len(pid) - 1, NUM_NESTED_LEVELS) for pid in pos_strings_raw] + [0]\n",
    "        rel_pos_gt = [0] + [1 if pid.endswith('L') else 2 if pid.endswith('R') else 0 for pid in pos_strings_raw] + [0]\n",
    "        \n",
    "        # 4. Кодирование строк позиций в числовые ID и создание позиционной матрицы\n",
    "        pos_identifiers_ids = []\n",
    "        # Позиция для SOS токена (стандартная)\n",
    "        sos_pos_ids = [self.pos_vocab.sos_id] + [self.pos_vocab.stoi['M']] + [self.pos_vocab.stoi['<EOS>']]\n",
    "        pos_identifiers_ids.append(sos_pos_ids)\n",
    "        # Позиции для каждого токена формулы\n",
    "        for pos_str in pos_strings_raw:\n",
    "            ids = [self.pos_vocab.stoi.get(c, 0) for c in pos_str]\n",
    "            ids = [self.pos_vocab.sos_id] + ids + [self.pos_vocab.stoi['<EOS>']]\n",
    "            pos_identifiers_ids.append(ids)\n",
    "        # Позиция для EOS токена\n",
    "        pos_identifiers_ids.append(sos_pos_ids)\n",
    "        \n",
    "        # 5. Паддинг всех последовательностей до единой длины\n",
    "        final_len = len(tokens_gt)\n",
    "        tokens_gt_padded = tokens_gt + [PAD_TOKEN] * (self.max_len - final_len)\n",
    "        nested_depth_gt_padded = nested_depth_gt + [0] * (self.max_len - final_len)\n",
    "        rel_pos_gt_padded = rel_pos_gt + [0] * (self.max_len - final_len)\n",
    "        \n",
    "        # Паддинг каждой строки в позиционной матрице до MAX_IDENTIFIER_LEN\n",
    "        for i in range(len(pos_identifiers_ids)):\n",
    "            seq = pos_identifiers_ids[i][:self.max_identifier_len]\n",
    "            pos_identifiers_ids[i] = seq + [self.pos_vocab.pad_id] * (self.max_identifier_len - len(seq))\n",
    "        \n",
    "        # Паддинг самой матрицы (добавление пустых строк для паддинга токенов)\n",
    "        empty_pos_id_seq = [self.pos_vocab.pad_id] * self.max_identifier_len\n",
    "        padded_pos_ids = pos_identifiers_ids + [empty_pos_id_seq] * (self.max_len - final_len)\n",
    "        \n",
    "        # Возвращаем словарь с готовыми к отправке в модель тензорами\n",
    "        return {\n",
    "            \"tokens_gt\": tokens_gt_padded,\n",
    "            \"pos_matrix\": torch.tensor(padded_pos_ids, dtype=torch.long),\n",
    "            \"nested_gt\": torch.tensor(nested_depth_gt_padded, dtype=torch.long),\n",
    "            \"rel_pos_gt\": torch.tensor(rel_pos_gt_padded, dtype=torch.long)\n",
    "        }\n",
    "# =================================================================================\n",
    "#               3. КЛАССЫ ДЛЯ ПРЕ-ОБРАБОТКИ ИЗОБРАЖЕНИЙ И DATASET\n",
    "# =================================================================================\n",
    "\n",
    "# В этой версии кода используется более гибкий подход к аугментации и изменению размера.\n",
    "# Вместо одного `transforms.Compose` в `__init__`, трансформации применяются\n",
    "# вручную и последовательно в методе `__getitem__`. Это дает больше контроля.\n",
    "\n",
    "class ResizeWithPadding:\n",
    "    \"\"\"\n",
    "    Класс для изменения размера изображения до целевой высоты с сохранением пропорций\n",
    "    и последующим добавлением паддинга до максимальной ширины.\n",
    "    (Этот класс присутствует в коде, но в `CROHMEDataset` не используется,\n",
    "     вместо него применяется более гибкая связка ScaleToLimitRange + ручной паддинг в collate_fn)\n",
    "    \"\"\"\n",
    "    def __init__(self, target_height, max_target_width, padding_value=255):\n",
    "        self.target_height = target_height\n",
    "        self.max_target_width = max_target_width\n",
    "        self.padding_value = padding_value\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        # Вычисляем новую ширину, сохраняя пропорции\n",
    "        new_w = int(w * (self.target_height / h))\n",
    "        # Ограничиваем максимальную ширину\n",
    "        new_w = min(new_w, self.max_target_width)\n",
    "        img_resized = img.resize((new_w, self.target_height), Image.LANCZOS)\n",
    "        \n",
    "        # Создаем новое \"пустое\" изображение (холст) с целевыми размерами\n",
    "        new_img = Image.new(img.mode, (self.max_target_width, self.target_height), self.padding_value)\n",
    "        # Вставляем измененное изображение в левый верхний угол холста\n",
    "        new_img.paste(img_resized, (0, 0))\n",
    "        \n",
    "        # Создаем маску для паддинга, чтобы модель знала, где реальное изображение, а где пустота\n",
    "        mask = torch.ones((self.target_height, self.max_target_width), dtype=torch.bool)\n",
    "        mask[:, :new_w] = False # False - реальные пиксели, True - паддинг\n",
    "        return new_img, mask\n",
    "\n",
    "class ScaleToLimitRange:\n",
    "    \"\"\"\n",
    "    Трансформация, которая пропорционально изменяет размер изображения так,\n",
    "    чтобы его высота и ширина попали в заданные диапазоны [h_lo, h_hi] и [w_lo, w_hi].\n",
    "    Работает с numpy-массивами.\n",
    "    \"\"\"\n",
    "    def __init__(self, w_lo: int, w_hi: int, h_lo: int, h_hi: int) -> None:\n",
    "        assert w_lo <= w_hi and h_lo <= h_hi\n",
    "        self.w_lo = w_lo\n",
    "        self.w_hi = w_hi\n",
    "        self.h_lo = h_lo\n",
    "        self.h_hi = h_hi\n",
    "\n",
    "    def __call__(self, img: np.ndarray) -> np.ndarray:\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Если изображение слишком большое хотя бы по одному измерению, сжимаем его\n",
    "        scale_r = min(self.h_hi / h, self.w_hi / w)\n",
    "        if scale_r < 1.0:\n",
    "            img = cv2.resize(img, None, fx=scale_r, fy=scale_r, interpolation=cv2.INTER_LINEAR)\n",
    "            return img\n",
    "\n",
    "        # Если изображение слишком маленькое хотя бы по одному измерению, растягиваем его\n",
    "        scale_r = max(self.h_lo / h, self.w_lo / w)\n",
    "        if scale_r > 1.0:\n",
    "            img = cv2.resize(img, None, fx=scale_r, fy=scale_r, interpolation=cv2.INTER_LINEAR)\n",
    "            return img\n",
    "\n",
    "        # Если размеры уже в пределах нормы, ничего не делаем\n",
    "        return img\n",
    "        \n",
    "class ScaleAugmentation:\n",
    "    \"\"\"\n",
    "    Трансформация для аугментации данных: случайное пропорциональное\n",
    "    изменение масштаба изображения в заданном диапазоне [lo, hi].\n",
    "    Применяется только на этапе обучения.\n",
    "    \"\"\"\n",
    "    def __init__(self, lo: float, hi: float) -> None:\n",
    "        assert lo <= hi\n",
    "        self.lo = lo\n",
    "        self.hi = hi\n",
    "\n",
    "    def __call__(self, img: np.ndarray) -> np.ndarray:\n",
    "        # Выбираем случайный коэффициент масштабирования\n",
    "        k = np.random.uniform(self.lo, self.hi)\n",
    "        img = cv2.resize(img, None, fx=k, fy=k, interpolation=cv2.INTER_LINEAR)\n",
    "        return img\n",
    "\n",
    "class CROHMEDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Основной класс датасета. Загружает изображения и соответствующие им LaTeX-формулы,\n",
    "    применяет трансформации и подготавливает данные для модели.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dir, caption_file, vocab, pos_vocab, is_train=True):\n",
    "        self.vocab = vocab\n",
    "        self.pfe = PositionForestEncoder(pos_vocab) # Инициализируем наш парсер позиций\n",
    "        self.items = []\n",
    "        self.is_train = is_train\n",
    "\n",
    "        # Формируем список пар (путь_к_изображению, строка_latex)\n",
    "        img_dir = os.path.join(base_dir, 'img')\n",
    "        formulas_path = os.path.join(base_dir, caption_file)\n",
    "        with open(formulas_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                fid, latex = line.strip().split('\\t')\n",
    "                self.items.append((os.path.join(img_dir, f\"{fid}.bmp\"), latex))\n",
    "\n",
    "        # Определяем диапазоны размеров и инициализируем классы трансформаций\n",
    "        H_MIN, H_MAX = 32, 256\n",
    "        W_MIN, W_MAX = 32, 512\n",
    "        \n",
    "        self.scale_augmenter = None\n",
    "        if self.is_train:\n",
    "            # Аугментация масштаба применяется только для обучающей выборки\n",
    "            self.scale_augmenter = ScaleAugmentation(0.8, 1.2)\n",
    "        \n",
    "        # Контроллер размера применяется всегда (и для train, и для val)\n",
    "        self.size_controller = ScaleToLimitRange(h_lo=H_MIN, h_hi=H_MAX, w_lo=W_MIN, w_hi=W_MAX)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, latex = self.items[idx]\n",
    "        try:\n",
    "            # Загружаем изображение с помощью OpenCV в виде numpy-массива в оттенках серого\n",
    "            img_np = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img_np is None: \n",
    "                raise FileNotFoundError(f\"Не удалось прочитать файл: {path}\")\n",
    "        except Exception:\n",
    "            # В случае ошибки (битый файл, и т.д.) рекурсивно берем следующий элемент датасета\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "            \n",
    "        # --- Ручной пайплайн применения трансформаций ---\n",
    "        # Этот подход дает больше гибкости, чем стандартный `transforms.Compose`.\n",
    "\n",
    "        # Шаг 1: Аугментация масштаба (только для train)\n",
    "        if self.scale_augmenter:\n",
    "            img_np = self.scale_augmenter(img_np)\n",
    "        \n",
    "        # Шаг 2: Приведение к целевому диапазону размеров\n",
    "        img_np = self.size_controller(img_np)\n",
    "\n",
    "        # Шаг 3: Конвертация в PIL.Image для дальнейшей обработки в DataLoader'е\n",
    "        final_img_pil = Image.fromarray(img_np)\n",
    "        \n",
    "        # --- Обработка LaTeX-формулы ---\n",
    "        \n",
    "        # Получаем словарь с тензорами позиций от нашего парсера\n",
    "        data = self.pfe.process_formula(latex)\n",
    "        # Кодируем ground-truth токены в их ID\n",
    "        token_ids = torch.tensor(self.vocab.encode(data[\"tokens_gt\"]), dtype=torch.long)\n",
    "        \n",
    "        # Возвращаем все необходимые данные. collate_fn позже соберет их в батч.\n",
    "        return (final_img_pil, token_ids, data[\"pos_matrix\"], data[\"nested_gt\"], data[\"rel_pos_gt\"], latex)\n",
    "\n",
    "# =================================================================================\n",
    "#                         4. ЭНКОДЕР (КАСТОМНАЯ РЕАЛИЗАЦИЯ DenseNet)\n",
    "# =================================================================================\n",
    "\n",
    "class ImgPosEnc(nn.Module):\n",
    "    \"\"\"\n",
    "    Создает 2D позиционные кодировки для фичер-мапы изображения, аналогично\n",
    "    позиционным кодировкам в трансформерах, но для двух измерений (высота и ширина).\n",
    "    Это позволяет модели понимать пространственное расположение признаков.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, temperature: float = 10000.0, normalize: bool = True, scale: float = None):\n",
    "        super().__init__()\n",
    "        if d_model % 4 != 0: raise ValueError(f\"d_model ({d_model}) должен быть кратен 4.\")\n",
    "        self.d_model = d_model\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "        if scale is None: scale = 2 * math.pi\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.BoolTensor) -> torch.Tensor:\n",
    "        # `mask` здесь - это маска паддинга (False - где есть пиксели, True - где паддинг)\n",
    "        not_mask = ~mask\n",
    "        # Создаем координатные сетки: y_embed - вертикальные, x_embed - горизонтальные\n",
    "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
    "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
    "        \n",
    "        if self.normalize:\n",
    "            # Нормализуем координаты в диапазон [0, scale] для стабильности\n",
    "            eps = 1e-6\n",
    "            y_embed = (y_embed / (y_embed[:, -1:, :] + eps)) * self.scale\n",
    "            x_embed = (x_embed / (x_embed[:, :, -1:] + eps)) * self.scale\n",
    "\n",
    "        # Формула позиционного кодирования (sin/cos на разных частотах)\n",
    "        dim_t_half = self.d_model // 2\n",
    "        dim_t = torch.arange(dim_t_half, dtype=torch.float32, device=x.device)\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / dim_t_half)\n",
    "        \n",
    "        pos_x = x_embed[:, :, :, None] / dim_t\n",
    "        pos_y = y_embed[:, :, :, None] / dim_t\n",
    "        \n",
    "        # Чередуем sin и cos\n",
    "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        \n",
    "        # Конкатенируем кодировки по X и Y, и прибавляем к входному тензору\n",
    "        pos = torch.cat((pos_y, pos_x), dim=3)\n",
    "        return x + pos\n",
    "\n",
    "class _Bottleneck(nn.Module):\n",
    "    \"\"\"Внутренний блок \"бутылочное горлышко\" для DenseNet.\"\"\"\n",
    "    def __init__(self, n_channels: int, growth_rate: int, use_dropout: bool):\n",
    "        super(_Bottleneck, self).__init__()\n",
    "        interChannels = 4 * growth_rate\n",
    "        # Сначала 1x1 свертка для уменьшения каналов (узкое место)\n",
    "        self.conv1 = nn.Conv2d(n_channels, interChannels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(interChannels)\n",
    "        # Затем основная 3x3 свертка\n",
    "        self.conv2 = nn.Conv2d(interChannels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(growth_rate)\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        if self.use_dropout: out = self.dropout(out)\n",
    "        out = F.relu(self.bn2(self.conv2(out)), inplace=True)\n",
    "        if self.use_dropout: out = self.dropout(out)\n",
    "        # Ключевая идея DenseNet: конкатенация входа и выхода по канальному измерению\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class _Transition(nn.Module):\n",
    "    \"\"\"Переходный слой в DenseNet. Уменьшает количество каналов и пространственный размер.\"\"\"\n",
    "    def __init__(self, n_channels: int, n_out_channels: int, use_dropout: bool):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(n_channels, n_out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(n_out_channels)\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n",
    "        if self.use_dropout: out = self.dropout(out)\n",
    "        # Уменьшение пространственного размера (downsampling)\n",
    "        out = F.avg_pool2d(out, 2, ceil_mode=True)\n",
    "        return out\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    \"\"\"Кастомная реализация архитектуры DenseNet, адаптированная для задачи.\"\"\"\n",
    "    def __init__(self, growth_rate: int, num_layers: int, reduction: float = 0.5, bottleneck: bool = True, use_dropout: bool = True):\n",
    "        super(DenseNet, self).__init__()\n",
    "        n_dense_blocks = num_layers\n",
    "        n_channels = 2 * growth_rate\n",
    "        \n",
    "        # Первый сверточный слой\n",
    "        self.conv1 = nn.Conv2d(1, n_channels, kernel_size=7, padding=3, stride=2, bias=False) # 1 канал на входе (grayscale)\n",
    "        self.norm1 = nn.BatchNorm2d(n_channels)\n",
    "        \n",
    "        # Первый блок плотных слоев\n",
    "        self.dense1 = self._make_dense(n_channels, growth_rate, n_dense_blocks, bottleneck, use_dropout)\n",
    "        n_channels += n_dense_blocks * growth_rate\n",
    "        n_out_channels = int(math.floor(n_channels * reduction))\n",
    "        self.trans1 = _Transition(n_channels, n_out_channels, use_dropout)\n",
    "        \n",
    "        # Второй блок плотных слоев\n",
    "        n_channels = n_out_channels\n",
    "        self.dense2 = self._make_dense(n_channels, growth_rate, n_dense_blocks, bottleneck, use_dropout)\n",
    "        n_channels += n_dense_blocks * growth_rate\n",
    "        n_out_channels = int(math.floor(n_channels * reduction))\n",
    "        self.trans2 = _Transition(n_channels, n_out_channels, use_dropout)\n",
    "        \n",
    "        # Третий блок плотных слоев\n",
    "        n_channels = n_out_channels\n",
    "        self.dense3 = self._make_dense(n_channels, growth_rate, n_dense_blocks, bottleneck, use_dropout)\n",
    "        self.out_channels = n_channels + n_dense_blocks * growth_rate\n",
    "        self.post_norm = nn.BatchNorm2d(self.out_channels)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_dense(n_channels, growth_rate, n_dense_blocks, bottleneck, use_dropout):\n",
    "        \"\"\"Собирает последовательность из нескольких плотных блоков.\"\"\"\n",
    "        layers = []\n",
    "        for _ in range(int(n_dense_blocks)):\n",
    "            if bottleneck:\n",
    "                layers.append(_Bottleneck(n_channels, growth_rate, use_dropout))\n",
    "            n_channels += growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x, x_mask):\n",
    "        # Прогоняем изображение через всю сеть, корректно изменяя маску на каждом шаге downsampling'а\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out_mask = x_mask[:, ::2, ::2]\n",
    "        \n",
    "        out = F.relu(out, inplace=True)\n",
    "        out = F.max_pool2d(out, 2, ceil_mode=True)\n",
    "        out_mask = out_mask[:, ::2, ::2]\n",
    "        \n",
    "        out = self.dense1(out)\n",
    "        out = self.trans1(out)\n",
    "        out_mask = out_mask[:, ::2, ::2]\n",
    "        \n",
    "        out = self.dense2(out)\n",
    "        out = self.trans2(out)\n",
    "        out_mask = out_mask[:, ::2, ::2]\n",
    "        \n",
    "        out = self.dense3(out)\n",
    "        out = self.post_norm(out)\n",
    "        return out, out_mask\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Основной класс энкодера, который объединяет DenseNet для извлечения признаков\n",
    "    и 2D позиционное кодирование.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, growth_rate, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.densenet = DenseNet(growth_rate, num_layers)\n",
    "        # Проекция из количества каналов DenseNet в рабочую размерность модели d_model\n",
    "        self.feature_proj = nn.Conv2d(self.densenet.out_channels, d_model, 1)\n",
    "        self.pos_enc_2d = ImgPosEnc(d_model, normalize=True)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, img, img_mask):\n",
    "        # 1. Извлекаем фичер-мапу из DenseNet\n",
    "        feature, mask = self.densenet(img, img_mask)\n",
    "        # 2. Проецируем в нужное кол-во каналов\n",
    "        feature = self.feature_proj(feature)\n",
    "        \n",
    "        # 3. Добавляем 2D позиционные кодировки\n",
    "        feature_permuted = feature.permute(0, 2, 3, 1) # Переставляем размерности для LayerNorm\n",
    "        pos_encoded_feature = self.pos_enc_2d(feature_permuted, mask)\n",
    "        \n",
    "        # 4. Нормализация и dropout\n",
    "        normed_feature = self.norm(pos_encoded_feature)\n",
    "        dropped_feature = self.dropout(normed_feature)\n",
    "\n",
    "        # 5. \"Выравниваем\" 2D фичер-мапу в 1D последовательность для декодера-трансформера\n",
    "        return rearrange(dropped_feature, \"b h w d -> b (h w) d\"), rearrange(mask, \"b h w -> b (h w)\")\n",
    "\n",
    "# =================================================================================\n",
    "#                     5. ДЕКОДЕР И МОДУЛЬ КОРРЕКЦИИ ВНИМАНИЯ (IAC)\n",
    "# =================================================================================\n",
    "\n",
    "class StandardCrossAttention(nn.Module):\n",
    "    \"\"\"Обертка для стандартного Multi-Head Attention, чтобы его интерфейс совпадал с IAC.\"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "    def forward(self, q, k, v, ids, pad_mask): # `ids` здесь не используется, но нужен для совместимости\n",
    "        return self.mha(q, k, v, key_padding_mask=pad_mask)[0]\n",
    "\n",
    "class IAC(nn.Module):\n",
    "    \"\"\"\n",
    "    Implicit Attention Correction. Продвинутый механизм внимания, который пытается\n",
    "    решить проблему \"пере-внимания\" на уже распознанные участки, особенно для\n",
    "    структурных символов, у которых нет своего изображения.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout, vocab):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.wq, self.wk, self.wv, self.wo = [nn.Linear(d_model, d_model, bias=False) for _ in range(4)]\n",
    "        # Свертка и линейный слой для модуля коррекции phi\n",
    "        self.phi_conv = nn.Conv2d(num_heads, num_heads, kernel_size=3, padding=1, groups=num_heads)\n",
    "        self.phi_linear = nn.Linear(self.num_heads, self.head_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Собираем ID всех \"структурных\" токенов, которые не имеют прямого аналога на картинке\n",
    "        structure_symbols = {\n",
    "            '^', '_', '{', '}', '\\\\frac', '\\\\sqrt', '\\\\left', '\\\\right',\n",
    "            '\\\\big', '\\\\Big', '\\\\bigg', '\\\\Bigg', '&', '\\\\\\\\',\n",
    "            PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN\n",
    "        }\n",
    "        s_ids = [vocab.stoi[s] for s in structure_symbols if s in vocab.stoi]\n",
    "        self.register_buffer('structure_ids', torch.tensor(s_ids, dtype=torch.long))\n",
    "\n",
    "    def _compute_phi(self, accum_attn):\n",
    "        \"\"\"Вспомогательная функция для вычисления корректирующего члена phi.\"\"\"\n",
    "        B, H, Lq, Lk = accum_attn.shape\n",
    "        s_dim_float = math.sqrt(Lk)\n",
    "        # Эта реализация предполагает, что фичер-мапу можно \"свернуть\" в квадрат\n",
    "        if s_dim_float != int(s_dim_float): return 0\n",
    "        s_dim = int(s_dim_float)\n",
    "\n",
    "        # Применяем 2D свертку к накопленным весам внимания\n",
    "        phi_in = rearrange(accum_attn, 'b h lq (s1 s2) -> (b lq) h s1 s2', s1=s_dim, s2=s_dim)\n",
    "        conv_out = self.phi_conv(phi_in)\n",
    "        phi_permuted = rearrange(conv_out, '(b lq) h s1 s2 -> b lq (s1 s2) h', b=B)\n",
    "        phi_features = self.phi_linear(phi_permuted)\n",
    "        return phi_features.unsqueeze(1)\n",
    "\n",
    "    def forward(self, q, k, v, ids, pad_mask):\n",
    "        B, Lq, _ = q.shape; Lk = k.shape[1]\n",
    "        \n",
    "        # Стандартные проекции Q, K, V\n",
    "        Q = self.wq(q).view(B, Lq, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.wk(k).view(B, Lk, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.wv(v).view(B, Lk, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # --- Логика IAC ---\n",
    "        # 1. Получаем временные веса внимания (без градиента)\n",
    "        temp_attn_weights = F.softmax(scores, dim=-1).detach()\n",
    "        # 2. Создаем индикатор, который равен 1 для обычных символов и 0 для структурных\n",
    "        indicator = (~(ids.unsqueeze(-1) == self.structure_ids).any(-1)).float().view(B, 1, Lq, 1)\n",
    "        # 3. \"Обнуляем\" внимание от структурных символов\n",
    "        masked_attn = temp_attn_weights * indicator\n",
    "        # 4. Накапливаем \"правильное\" внимание с предыдущих шагов\n",
    "        shifted = torch.zeros_like(masked_attn)\n",
    "        if Lq > 1: shifted[:, :, 1:, :] = masked_attn[:, :, :-1, :]\n",
    "        accumulated_attention = torch.cumsum(shifted, dim=2)\n",
    "\n",
    "        # 5. Вычисляем и вычитаем корректирующий член\n",
    "        phi = self._compute_phi(accumulated_attention)\n",
    "        correction = (Q.unsqueeze(3) * phi).sum(dim=-1) if isinstance(phi, torch.Tensor) else 0\n",
    "        corrected_scores = scores - correction\n",
    "\n",
    "        # Стандартное завершение\n",
    "        if pad_mask is not None:\n",
    "            corrected_scores = corrected_scores.masked_fill(pad_mask.unsqueeze(1).unsqueeze(2), float('-inf'))\n",
    "        final_attn_weights = F.softmax(corrected_scores, dim=-1)\n",
    "        context = torch.matmul(self.dropout(final_attn_weights), V).transpose(1, 2).reshape(B, Lq, self.d_model)\n",
    "        return self.wo(context)\n",
    "\n",
    "class EnhancedDecoderLayer(nn.Module):\n",
    "    \"\"\"Один слой декодера, который включает в себя self-attention, cross-attention (обычный или IAC) и FFN.\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout, cross_attention_module):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        # Используем переданный модуль, что позволяет гибко менять тип cross-attention\n",
    "        self.cross_attn = cross_attention_module\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_ff), nn.GELU(), nn.Linear(d_ff, d_model))\n",
    "        self.norm1, self.norm2, self.norm3 = [nn.LayerNorm(d_model) for _ in range(3)]\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, ids, tgt_mask, tgt_pad_mask, mem_pad_mask):\n",
    "        # 1. Self-attention (внимание на предыдущие сгенерированные токены)\n",
    "        x = x + self.dropout(self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), attn_mask=tgt_mask, key_padding_mask=tgt_pad_mask)[0])\n",
    "        # 2. Cross-attention (внимание на выход энкодера)\n",
    "        x = x + self.dropout(self.cross_attn(self.norm2(x), enc_out, enc_out, ids, mem_pad_mask))\n",
    "        # 3. Feed-forward network\n",
    "        x = x + self.dropout(self.ffn(self.norm3(x)))\n",
    "        return x\n",
    "\n",
    "# =================================================================================\n",
    "#                      6. ПОЛНАЯ МОДЕЛЬ POSFORMER И ВСПОМОГАТЕЛЬНЫЕ КЛАССЫ\n",
    "# =================================================================================\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    Реализация Cross-Entropy с \"сглаживанием меток\" (Label Smoothing).\n",
    "    Это мощный метод регуляризации, который не дает модели становиться\n",
    "    излишне самоуверенной в своих предсказаниях, что улучшает обобщение.\n",
    "    Вместо того чтобы требовать от модели предсказать [0, 0, 1, 0], мы просим ее\n",
    "    предсказать что-то вроде [0.01, 0.01, 0.95, 0.01].\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing=0.0):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "    def forward(self, x, target, ignore_index=-100):\n",
    "        confidence = 1. - self.smoothing\n",
    "        logprobs = F.log_softmax(x, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        # Игнорируем потери для паддинг-токенов\n",
    "        mask = (target != ignore_index)\n",
    "        return (loss * mask.float()).sum() / mask.float().sum()\n",
    "\n",
    "class PosFormer(nn.Module):\n",
    "    \"\"\"\n",
    "    Основной класс модели, собирающий все компоненты воедино.\n",
    "    \"\"\"\n",
    "    def __init__(self, main_vocab, pos_vocab):\n",
    "        super().__init__()\n",
    "        self.vocab, self.pos_vocab = main_vocab, pos_vocab\n",
    "        self.pad_id, self.sos_id, self.eos_id = main_vocab.pad_id, main_vocab.sos_id, main_vocab.eos_id\n",
    "        \n",
    "        # --- Компоненты модели ---\n",
    "        self.encoder = Encoder(D_MODEL, GROWTH_RATE, NUM_ENCODER_LAYERS, DROPOUT_RATE)\n",
    "        # Эмбеддинги для токенов LaTeX\n",
    "        self.emb_tok = nn.Embedding(len(main_vocab.itos), D_MODEL)\n",
    "        # Эмбеддинги для позиционных символов ('M', 'L', 'R')\n",
    "        self.pos_id_emb = nn.Embedding(len(pos_vocab.itos), D_MODEL)\n",
    "        # Линейная проекция для \"сплющенной\" матрицы позиций\n",
    "        self.xi_proj = nn.Sequential(nn.Linear(D_MODEL * MAX_IDENTIFIER_LEN, D_MODEL), nn.GELU(), nn.LayerNorm(D_MODEL))\n",
    "\n",
    "        # Создаем слои декодера: первый слой - обычный Cross-Attention, остальные - с IAC\n",
    "        decoder_layers = []\n",
    "        for i in range(NUM_DECODER_LAYERS):\n",
    "            use_iac = i > 0 # IAC на 2м и 3м слое\n",
    "            cross_attn_module = IAC(D_MODEL, NUM_HEADS, DROPOUT_RATE, main_vocab) if use_iac \\\n",
    "                else StandardCrossAttention(D_MODEL, NUM_HEADS, DROPOUT_RATE)\n",
    "            decoder_layers.append(\n",
    "                EnhancedDecoderLayer(D_MODEL, NUM_HEADS, D_FF, DROPOUT_RATE, cross_attn_module)\n",
    "            )\n",
    "        self.decoders = nn.ModuleList(decoder_layers)\n",
    "\n",
    "        self.dec_norm = nn.LayerNorm(D_MODEL)\n",
    "        \n",
    "        # --- \"Головы\" для предсказания (три задачи) ---\n",
    "        self.head_tok = nn.Linear(D_MODEL, len(main_vocab.itos))      # 1. Предсказание следующего токена LaTeX\n",
    "        self.head_nested = nn.Linear(D_MODEL, NUM_NESTED_LEVELS + 1) # 2. Предсказание уровня вложенности\n",
    "        self.head_rel_pos = nn.Linear(D_MODEL, 3)                    # 3. Предсказание относительной позиции (M, L, R)\n",
    "\n",
    "        # Функция потерь с Label Smoothing для основной задачи\n",
    "        self.loss_fn_rec = LabelSmoothingCrossEntropy(smoothing=0.05)\n",
    "        self._init_weights()\n",
    "        self.rel_pos_map = {0: 'M', 1: 'L', 2: 'R'}\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Инициализация весов для лучшей сходимости.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, imgs, img_mask, pos_matrix, token_ids):\n",
    "        # 1. Получаем фичи из энкодера\n",
    "        vis_feats, mem_pad_mask = self.encoder(imgs, img_mask)\n",
    "        \n",
    "        # 2. Готовим вход для декодера\n",
    "        # 2.1. Получаем эмбеддинги для позиций\n",
    "        pos_input = self.xi_proj(rearrange(self.pos_id_emb(pos_matrix[:, :-1, :]), 'b l d e -> b l (d e)'))\n",
    "        # 2.2. Получаем эмбеддинги для токенов\n",
    "        token_input_ids = token_ids[:, :-1]\n",
    "        token_embeds = self.emb_tok(token_input_ids)\n",
    "        # 2.3. Суммируем эмбеддинги токенов и позиций\n",
    "        tgt = pos_input + token_embeds\n",
    "        \n",
    "        # 3. Готовим маски для декодера\n",
    "        # Маска, чтобы декодер не \"подсматривал\" в будущее\n",
    "        tgt_mask = torch.triu(torch.ones(tgt.size(1), tgt.size(1), device=tgt.device), 1).bool()\n",
    "        # Маска для паддинг-токенов в последовательности\n",
    "        tgt_pad_mask = (token_input_ids == self.pad_id)\n",
    "        \n",
    "        # 4. Прогоняем через декодер\n",
    "        dec_out = self.dec_norm(self._decode(tgt, vis_feats, token_input_ids, tgt_mask, tgt_pad_mask, mem_pad_mask))\n",
    "        \n",
    "        # 5. Получаем предсказания из голов\n",
    "        return self.head_tok(dec_out), self.head_nested(dec_out), self.head_rel_pos(dec_out)\n",
    "\n",
    "    def _decode(self, tgt, mem, ids, tgt_mask, tgt_pad_mask, mem_pad_mask):\n",
    "        \"\"\"Пропускает данные через все слои декодера.\"\"\"\n",
    "        for layer in self.decoders:\n",
    "            tgt = layer(tgt, mem, ids, tgt_mask, tgt_pad_mask, mem_pad_mask)\n",
    "        return tgt\n",
    "\n",
    "    def compute_loss(self, logits, targets):\n",
    "        \"\"\"Вычисляет общую потерю как сумму потерь по трем задачам.\"\"\"\n",
    "        token_logits, nested_logits, rel_pos_logits = logits\n",
    "        token_ids_gt, nested_gt, rel_pos_gt = targets\n",
    "        token_targets = token_ids_gt[:, 1:]\n",
    "        nested_targets = nested_gt[:, 1:]\n",
    "        rel_pos_targets = rel_pos_gt[:, 1:]\n",
    "\n",
    "        # Потери для основной задачи распознавания (с Label Smoothing)\n",
    "        loss_rec = self.loss_fn_rec(\n",
    "            token_logits.reshape(-1, token_logits.size(-1)),\n",
    "            token_targets.reshape(-1),\n",
    "            ignore_index=self.pad_id\n",
    "        )\n",
    "        \n",
    "        # Вычисляем потери для вспомогательных задач только для не-паддинг токенов\n",
    "        mask = (token_targets != self.pad_id).flatten()\n",
    "        if not mask.any(): # Если в батче только паддинг\n",
    "            return {'total': loss_rec, 'rec': loss_rec, 'pos': torch.tensor(0.0, device=token_logits.device)}\n",
    "\n",
    "        loss_nested = F.cross_entropy(nested_logits.reshape(-1, nested_logits.size(-1))[mask],\n",
    "                                      nested_targets.reshape(-1)[mask])\n",
    "        loss_rel = F.cross_entropy(rel_pos_logits.reshape(-1, rel_pos_logits.size(-1))[mask],\n",
    "                                   rel_pos_targets.reshape(-1)[mask])\n",
    "        loss_pos = loss_nested + loss_rel\n",
    "        \n",
    "        # Общая потеря - взвешенная сумма (здесь веса = 1)\n",
    "        total_loss = loss_rec + loss_pos\n",
    "\n",
    "        return {'total': total_loss, 'rec': loss_rec, 'pos': loss_pos}\n",
    "    def _construct_next_pos_string(self, prev_pos_str: str, pred_nested_level: int, pred_rel_pos: int) -> str:\n",
    "        prev_level = len(prev_pos_str) - 1\n",
    "        new_pos_str = prev_pos_str[:pred_nested_level + 1]\n",
    "        if pred_nested_level > prev_level:\n",
    "            if len(new_pos_str) == prev_level + 1:\n",
    "                 new_pos_str += self.rel_pos_map.get(pred_rel_pos, 'M')\n",
    "        return new_pos_str\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, imgs, max_gen_len=150):\n",
    "        self.eval()\n",
    "        B = imgs.shape[0]; device = imgs.device\n",
    "        img_mask = torch.zeros_like(imgs[:, 0, :, :], dtype=torch.bool)\n",
    "        vis_feats, mem_pad_mask = self.encoder(imgs, img_mask)\n",
    "        generated_ids = torch.full((B, 1), self.sos_id, dtype=torch.long, device=device)\n",
    "        pos_strings_T = [['M'] for _ in range(B)]\n",
    "        is_finished = torch.zeros(B, dtype=torch.bool, device=device)\n",
    "        for t in range(max_gen_len - 1):\n",
    "            token_embeds = self.emb_tok(generated_ids)\n",
    "            pos_ids_list = []\n",
    "            max_len_in_batch = max(len(p_list) for p_list in pos_strings_T)\n",
    "            for i in range(B):\n",
    "                batch_pos_ids = []\n",
    "                for p_str in pos_strings_T[i]:\n",
    "                    ids = [self.pos_vocab.sos_id] + [self.pos_vocab.stoi.get(c,0) for c in p_str] + [self.pos_vocab.stoi['<EOS>']]\n",
    "                    padded_ids = ids[:MAX_IDENTIFIER_LEN] + [self.pos_vocab.pad_id] * (MAX_IDENTIFIER_LEN - len(ids))\n",
    "                    batch_pos_ids.append(torch.tensor(padded_ids, device=device))\n",
    "                while len(batch_pos_ids) < max_len_in_batch:\n",
    "                    batch_pos_ids.append(torch.full((MAX_IDENTIFIER_LEN,), self.pos_vocab.pad_id, device=device))\n",
    "                pos_ids_list.append(torch.stack(batch_pos_ids))\n",
    "            pos_matrix = torch.stack(pos_ids_list)\n",
    "            pos_embeds = self.xi_proj(rearrange(self.pos_id_emb(pos_matrix), 'b l d e -> b l (d e)'))\n",
    "            tgt = token_embeds + pos_embeds\n",
    "            tgt_mask = torch.triu(torch.ones(tgt.size(1), tgt.size(1), device=device), 1).bool()\n",
    "            dec_out = self.dec_norm(self._decode(tgt, vis_feats, generated_ids, tgt_mask, None, mem_pad_mask))\n",
    "            last_step_out = dec_out[:, -1, :]\n",
    "            token_logits, nested_logits, rel_pos_logits = self.head_tok(last_step_out), self.head_nested(last_step_out), self.head_rel_pos(last_step_out)\n",
    "            next_token_id = torch.argmax(token_logits, dim=-1).unsqueeze(1)\n",
    "            pred_nested_level, pred_rel_pos = torch.argmax(nested_logits, dim=-1), torch.argmax(rel_pos_logits, dim=-1)\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "            for i in range(B):\n",
    "                if not is_finished[i]:\n",
    "                    new_pos_str = self._construct_next_pos_string(pos_strings_T[i][-1], pred_nested_level[i].item(), pred_rel_pos[i].item())\n",
    "                    pos_strings_T[i].append(new_pos_str)\n",
    "            is_finished |= (next_token_id.squeeze(-1) == self.eos_id)\n",
    "            if is_finished.all(): break\n",
    "        return generated_ids\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_beam_search(self, imgs, beam_size=5, max_gen_len=100):\n",
    "        self.eval(); B = imgs.shape[0]; device = imgs.device\n",
    "        img_mask = torch.zeros_like(imgs[:, 0, :, :], dtype=torch.bool)\n",
    "        vis_feats, mem_pad_mask = self.encoder(imgs, img_mask)\n",
    "        vis_feats = vis_feats.repeat_interleave(beam_size, dim=0)\n",
    "        if mem_pad_mask is not None: mem_pad_mask = mem_pad_mask.repeat_interleave(beam_size, dim=0)\n",
    "        effective_batch_size = B * beam_size\n",
    "        generated_ids = torch.full((effective_batch_size, 1), self.sos_id, dtype=torch.long, device=device)\n",
    "        pos_strings_T = [['M'] for _ in range(effective_batch_size)]\n",
    "        log_scores = torch.zeros(effective_batch_size, device=device)\n",
    "        is_finished = torch.zeros(effective_batch_size, dtype=torch.bool, device=device)\n",
    "        for t in range(max_gen_len - 1):\n",
    "            if is_finished.all(): break\n",
    "            token_embeds = self.emb_tok(generated_ids)\n",
    "            pos_ids_list = []\n",
    "            for i in range(effective_batch_size):\n",
    "                batch_pos_ids = []\n",
    "                for p_str in pos_strings_T[i]:\n",
    "                    ids = [self.pos_vocab.sos_id] + [self.pos_vocab.stoi.get(c, 0) for c in p_str] + [self.pos_vocab.stoi['<EOS>']]\n",
    "                    padded_ids = ids[:MAX_IDENTIFIER_LEN] + [self.pos_vocab.pad_id] * (MAX_IDENTIFIER_LEN - len(ids))\n",
    "                    batch_pos_ids.append(torch.tensor(padded_ids, device=device))\n",
    "                pos_ids_list.append(torch.stack(batch_pos_ids))\n",
    "            pos_matrix = torch.stack(pos_ids_list)\n",
    "            pos_embeds = self.xi_proj(rearrange(self.pos_id_emb(pos_matrix), 'b l d e -> b l (d e)'))\n",
    "            tgt = token_embeds + pos_embeds\n",
    "            tgt_mask = torch.triu(torch.ones(tgt.size(1), tgt.size(1), device=device), 1).bool()\n",
    "            dec_out = self.dec_norm(self._decode(tgt, vis_feats, generated_ids, tgt_mask, None, mem_pad_mask))\n",
    "            last_step_out = dec_out[:, -1, :]\n",
    "            token_logits, nested_logits, rel_pos_logits = self.head_tok(last_step_out), self.head_nested(last_step_out), self.head_rel_pos(last_step_out)\n",
    "            log_probs = F.log_softmax(token_logits, dim=-1)\n",
    "            if t > 0:\n",
    "                log_probs[is_finished] = -float('inf')\n",
    "                log_probs[is_finished, self.pad_id] = 0\n",
    "            total_scores = log_probs + log_scores.unsqueeze(1)\n",
    "            total_scores = total_scores.view(B, -1)\n",
    "            top_scores, top_indices = torch.topk(total_scores, beam_size, dim=1)\n",
    "            beam_indices = top_indices // len(self.vocab.itos)\n",
    "            token_indices = top_indices % len(self.vocab.itos)\n",
    "            batch_indices = torch.arange(B, device=device).view(-1, 1).repeat(1, beam_size)\n",
    "            beam_indices_abs = beam_indices + (batch_indices * beam_size)\n",
    "            generated_ids = generated_ids[beam_indices_abs.view(-1)]\n",
    "            pos_strings_T = [pos_strings_T[i] for i in beam_indices_abs.view(-1).tolist()]\n",
    "            generated_ids = torch.cat([generated_ids, token_indices.view(-1, 1)], dim=1)\n",
    "            pred_nested_levels = torch.argmax(nested_logits[beam_indices_abs.view(-1)], dim=-1)\n",
    "            pred_rel_poses = torch.argmax(rel_pos_logits[beam_indices_abs.view(-1)], dim=-1)\n",
    "            for i in range(effective_batch_size):\n",
    "                new_pos_str = self._construct_next_pos_string(pos_strings_T[i][-1], pred_nested_levels[i].item(), pred_rel_poses[i].item())\n",
    "                pos_strings_T[i].append(new_pos_str)\n",
    "            log_scores = top_scores.view(-1)\n",
    "            is_finished = is_finished[beam_indices_abs.view(-1)] | (token_indices.view(-1) == self.eos_id)\n",
    "        seq_lengths = (generated_ids != self.pad_id).sum(dim=1).float()\n",
    "        seq_lengths = torch.max(seq_lengths, torch.ones_like(seq_lengths))\n",
    "        normalized_scores = (log_scores / seq_lengths).view(B, beam_size)\n",
    "        best_beam_indices = torch.argmax(normalized_scores, dim=1)\n",
    "        final_indices = best_beam_indices + torch.arange(B, device=device) * beam_size\n",
    "        return generated_ids[final_indices]\n",
    "# =================================================================================\n",
    "#                                  7. TRAINING & EVALUATION (TEMPLATE)\n",
    "# =================================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===== Loss Functions =====\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, device, current_epoch=1, num_examples_to_print=5):\n",
    "    \"\"\"Оценивает модель с помощью генерации и сравнивает строки.\"\"\"\n",
    "    model.eval()\n",
    "    corr_seq = 0\n",
    "    total_seq = 0\n",
    "    eos_id = model.eos_id\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {current_epoch} Evaluating\")\n",
    "        for i, (imgs, _, _, _, _, _, latex_strings) in enumerate(pbar):\n",
    "            imgs = imgs.to(device)\n",
    "            #model.generate принимает только картинки\n",
    "            pred_tokens = model.generate_beam_search(imgs)\n",
    "\n",
    "            if i == 0: # Печатаем примеры только для первого батча\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(f\"====== EXAMPLES FROM VALIDATION (EPOCH {current_epoch}) ======\")\n",
    "                num_to_print = min(num_examples_to_print, imgs.size(0))\n",
    "                for j in range(num_to_print):\n",
    "                    pred_ids = pred_tokens[j].cpu().tolist()\n",
    "                    try:\n",
    "                        eos_index = pred_ids.index(eos_id)\n",
    "                        pred_ids = pred_ids[:eos_index]\n",
    "                    except ValueError: pass\n",
    "                    # Используем словарь из модели\n",
    "                    pred_str = model.vocab.decode(pred_ids)\n",
    "                    true_str = latex_strings[j]\n",
    "                    # Сравниваем строки без пробелов\n",
    "                    print(f\"\\n--- Example {j+1} ---\")\n",
    "                    print(f\"  GROUND TRUTH: {true_str.replace(' ', '')}\")\n",
    "                    print(f\"  PREDICTED   : {pred_str.replace(' ', '')}\")\n",
    "                print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "            for j in range(imgs.size(0)):\n",
    "                true_str_norm = latex_strings[j].replace(\" \", \"\")\n",
    "                pred_ids = pred_tokens[j].cpu().tolist()\n",
    "                try:\n",
    "                    eos_index = pred_ids.index(eos_id)\n",
    "                    pred_ids = pred_ids[:eos_index]\n",
    "                except ValueError: pass\n",
    "                pred_str_norm = model.vocab.decode(pred_ids).replace(\" \", \"\")\n",
    "\n",
    "                if pred_str_norm == true_str_norm:\n",
    "                    corr_seq += 1\n",
    "                total_seq += 1\n",
    "\n",
    "    seq_accuracy = corr_seq / max(total_seq, 1)\n",
    "    return {'sequence_accuracy': seq_accuracy}\n",
    "\n",
    "def create_optimizer_and_scheduler(model, total_steps, max_lr, weight_decay):\n",
    "    \"\"\"Создает AdamW и OneCycleLR scheduler.\"\"\"\n",
    "    no_decay = ['bias', 'norm']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=max_lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, total_steps=total_steps,\n",
    "                                                  pct_start=0.1, anneal_strategy='cos')\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def train(model, train_dataset, val_dataset, epochs, batch_size, device, save_path):\n",
    "    \"\"\"Основной цикл обучения модели.\"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    optimizer, scheduler = create_optimizer_and_scheduler(model, total_steps, MAX_LEARNING_RATE, WEIGHT_DECAY)\n",
    "\n",
    "    start_epoch, best_seq_acc = load_checkpoint(model, optimizer, scheduler, save_path, device)\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch} Train Loss: N/A\")\n",
    "\n",
    "        for batch_idx, (imgs, masks, token_ids, pos_matrix, nested_gt, rel_pos_gt, _) in enumerate(pbar):\n",
    "            imgs, masks, token_ids, pos_matrix, nested_gt, rel_pos_gt = \\\n",
    "                imgs.to(device), masks.to(device), token_ids.to(device), pos_matrix.to(device), nested_gt.to(device), rel_pos_gt.to(device)\n",
    "\n",
    "            #  Прямой вызов модели\n",
    "            logits = model(imgs, masks, pos_matrix, token_ids)\n",
    "            targets = (token_ids, nested_gt, rel_pos_gt)\n",
    "            # Считаем loss\n",
    "            losses = model.compute_loss(logits, targets)\n",
    "            loss = losses['total']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Хорошая практика от градиентного взрыва\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            # Подсчёт среднего лосса\n",
    "            pbar.set_description(f\"Epoch {epoch} Train Loss: {epoch_loss / (batch_idx + 1):.4f} | LR: {scheduler.get_last_lr()[0]:.1e}\")\n",
    "\n",
    "        # Подсчет среднего лосса за всю эпоху\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Валидация\n",
    "        if epoch>80:\n",
    "            metrics = evaluate_model(model, val_loader, device, current_epoch=epoch)\n",
    "            seq_acc = metrics['sequence_accuracy']\n",
    "            print(f\"\\nEpoch {epoch} Summary: Train Loss={avg_train_loss:.4f}, Val Seq Acc={seq_acc:.4f}\\n\")\n",
    "            if seq_acc > best_seq_acc:\n",
    "                best_seq_acc = seq_acc\n",
    "                save_checkpoint(model, optimizer, scheduler, epoch, best_seq_acc, save_path)\n",
    "        print(f\"\\nEpoch {epoch} Summary: Train Loss={avg_train_loss:.4f}\\n\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T14:15:59.640327Z",
     "iopub.status.busy": "2025-07-05T14:15:59.639786Z",
     "iopub.status.idle": "2025-07-05T14:15:59.822462Z",
     "shell.execute_reply": "2025-07-05T14:15:59.821074Z",
     "shell.execute_reply.started": "2025-07-05T14:15:59.640304Z"
    },
    "id": "FWVVVEZ6hZc5",
    "outputId": "5a567ddf-da65-45c6-ff93-6f1d7c062cc4",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 29\n"
     ]
    }
   ],
   "source": [
    "!rm best_model_onecyclelr_without_label_smoothing.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T11:19:22.807368Z",
     "iopub.status.busy": "2025-07-10T11:19:22.807075Z",
     "iopub.status.idle": "2025-07-10T11:19:22.812163Z",
     "shell.execute_reply": "2025-07-10T11:19:22.811360Z",
     "shell.execute_reply.started": "2025-07-10T11:19:22.807344Z"
    },
    "id": "DmdKmSdJZGhp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, best_acc, checkpoint_path):\n",
    "    \"\"\"Сохраняет состояние модели, оптимизатора и т.д. в файл.\"\"\"\n",
    "    print(f\"\\n==> Validation accuracy improved to {best_acc:.4f}. Saving checkpoint for epoch {epoch}...\")\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'best_val_acc': best_acc,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "    }\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print(f\"Чекпоинт сохранен в {checkpoint_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-10T11:19:24.542400Z",
     "iopub.status.busy": "2025-07-10T11:19:24.542137Z",
     "iopub.status.idle": "2025-07-10T11:19:24.548208Z",
     "shell.execute_reply": "2025-07-10T11:19:24.547306Z",
     "shell.execute_reply.started": "2025-07-10T11:19:24.542379Z"
    },
    "id": "5DKIyZjGmFna",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path, device):\n",
    "    \"\"\"Загружает состояние модели, оптимизатора и т.д. из файла.\"\"\"\n",
    "    start_epoch = 1\n",
    "    best_acc = 0.0\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"==> Загрузка чекпоинта из {checkpoint_path}...\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        if optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_acc = checkpoint.get('best_val_acc', 0.0)\n",
    "        print(f\"Чекпоинт загружен. Продолжаем с эпохи {start_epoch}, лучшая точность: {best_acc:.4f}\")\n",
    "    else:\n",
    "        print(\"==> Чекпоинт не найден. Обучение с нуля.\")\n",
    "    return start_epoch, best_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T06:50:37.414987Z",
     "iopub.status.busy": "2025-07-09T06:50:37.414310Z",
     "iopub.status.idle": "2025-07-09T06:50:37.426110Z",
     "shell.execute_reply": "2025-07-09T06:50:37.425385Z",
     "shell.execute_reply.started": "2025-07-09T06:50:37.414965Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2019', '2014', '2016', 'train']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-10T11:27:30.869363Z",
     "iopub.status.busy": "2025-07-10T11:27:30.869082Z",
     "iopub.status.idle": "2025-07-10T16:32:24.086913Z",
     "shell.execute_reply": "2025-07-10T16:32:24.085477Z",
     "shell.execute_reply.started": "2025-07-10T11:27:30.869342Z"
    },
    "id": "Iyi4Tzo9xhNX",
    "outputId": "b6c713d5-f94e-4303-e390-d11019451f0a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка данных для словаря...\n",
      "Создание словарей...\n",
      "Словарь сохранен в crohme_vocab.pkl, размер: 114 токенов.\n",
      "Создание датасетов...\n",
      "Размер обучающего набора: 8834, валидационного: 986\n",
      "Инициализация модели...\n",
      "==> Загрузка чекпоинта из posformer_best_v4_lr_off.pth...\n",
      "Чекпоинт загружен. Продолжаем с эпохи 59, лучшая точность: 0.1410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59 Train Loss: 0.8887 | LR: 2.9e-04: 100%|██████████| 1105/1105 [04:42<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59 Summary: Train Loss=0.8887\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60 Train Loss: 0.8900 | LR: 2.9e-04: 100%|██████████| 1105/1105 [04:43<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60 Summary: Train Loss=0.8900\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61 Train Loss: 0.8853 | LR: 2.9e-04: 100%|██████████| 1105/1105 [04:43<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 61 Summary: Train Loss=0.8853\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62 Train Loss: 0.8857 | LR: 2.9e-04: 100%|██████████| 1105/1105 [04:43<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62 Summary: Train Loss=0.8857\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63 Train Loss: 0.8813 | LR: 2.9e-04: 100%|██████████| 1105/1105 [04:43<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63 Summary: Train Loss=0.8813\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64 Train Loss: 0.8783 | LR: 2.9e-04: 100%|██████████| 1105/1105 [04:45<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64 Summary: Train Loss=0.8783\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65 Train Loss: 0.8787 | LR: 2.9e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 65 Summary: Train Loss=0.8787\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66 Train Loss: 0.8769 | LR: 2.9e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66 Summary: Train Loss=0.8769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67 Train Loss: 0.8727 | LR: 2.9e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67 Summary: Train Loss=0.8727\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68 Train Loss: 0.8808 | LR: 2.9e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68 Summary: Train Loss=0.8808\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69 Train Loss: 0.8960 | LR: 2.8e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 69 Summary: Train Loss=0.8960\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70 Train Loss: 0.8932 | LR: 2.8e-04: 100%|██████████| 1105/1105 [04:45<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70 Summary: Train Loss=0.8932\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71 Train Loss: 0.8924 | LR: 2.8e-04: 100%|██████████| 1105/1105 [04:45<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 71 Summary: Train Loss=0.8924\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72 Train Loss: 0.8912 | LR: 2.8e-04: 100%|██████████| 1105/1105 [04:45<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 72 Summary: Train Loss=0.8912\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73 Train Loss: 0.8856 | LR: 2.8e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73 Summary: Train Loss=0.8856\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74 Train Loss: 0.8846 | LR: 2.8e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 74 Summary: Train Loss=0.8846\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75 Train Loss: 0.8840 | LR: 2.8e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 75 Summary: Train Loss=0.8840\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76 Train Loss: 0.8816 | LR: 2.8e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 76 Summary: Train Loss=0.8816\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77 Train Loss: 0.8800 | LR: 2.8e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 77 Summary: Train Loss=0.8800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78 Train Loss: 0.8789 | LR: 2.8e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 78 Summary: Train Loss=0.8789\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79 Train Loss: 0.8765 | LR: 2.8e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79 Summary: Train Loss=0.8765\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80 Train Loss: 0.8773 | LR: 2.8e-04: 100%|██████████| 1105/1105 [04:43<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80 Summary: Train Loss=0.8773\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81 Train Loss: 0.8736 | LR: 2.7e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.88it/s]\n",
      "Epoch 81 Evaluating:   1%|          | 1/124 [00:01<02:49,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 81) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2k_{11}-m+m+m}\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{1+m+m}}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\frac{\\sqrt{1}+\\cos\\alpha}{1+\\tan^{2}}}}}\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2\\cos\\alpha\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : |\\frac{c_{n}\\sin\\alpha}+\\alpha}}{11+\\cos^{2}}\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81 Evaluating: 100%|██████████| 124/124 [03:54<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 81 Summary: Train Loss=0.8736, Val Seq Acc=0.1957\n",
      "\n",
      "\n",
      "==> Validation accuracy improved to 0.1957. Saving checkpoint for epoch 81...\n",
      "Чекпоинт сохранен в posformer_best_v4_lr_off.pth\n",
      "\n",
      "\n",
      "Epoch 81 Summary: Train Loss=0.8736\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82 Train Loss: 0.8733 | LR: 2.7e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.88it/s]\n",
      "Epoch 82 Evaluating:   1%|          | 1/124 [00:01<03:22,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 82) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2\\tan\\alpha\\tan\\alpha\\tan\\beta\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k=\\lim\\limits_{n\\rightarrow\\infty}|\\sin\\alpha-y\\sin(\\beta)\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\tan\\alpha\\tan\\beta\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2\\tan\\alpha\\tan\\alpha\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : |\\sinz|=2\\sinz|\\sinz|\\sinz|\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82 Evaluating: 100%|██████████| 124/124 [03:33<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 82 Summary: Train Loss=0.8733, Val Seq Acc=0.1592\n",
      "\n",
      "\n",
      "Epoch 82 Summary: Train Loss=0.8733\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83 Train Loss: 0.8717 | LR: 2.7e-04: 100%|██████████| 1105/1105 [04:43<00:00,  3.90it/s]\n",
      "Epoch 83 Evaluating:   1%|          | 1/124 [00:20<41:31, 20.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 83) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2k_{1+mum-2n}\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{n+1}u_{n}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\pi_{n+1}\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 20,\\alpha\\tan\\beta\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : |\\sinz|=\\frac{2}{1+yn}}|\\sinz\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83 Evaluating: 100%|██████████| 124/124 [08:57<00:00,  4.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 83 Summary: Train Loss=0.8717, Val Seq Acc=0.1765\n",
      "\n",
      "\n",
      "Epoch 83 Summary: Train Loss=0.8717\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84 Train Loss: 0.8685 | LR: 2.7e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.88it/s]\n",
      "Epoch 84 Evaluating:   1%|          | 1/124 [00:02<05:44,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 84) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2x_{1}+m+n+1}\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{1}+m+n+1}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -x_{1}+m+m+n}}\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2n_{1+1+1}}}}}\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : |\\sin\\frac{2}{1+y+1}}\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84 Evaluating: 100%|██████████| 124/124 [03:18<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 84 Summary: Train Loss=0.8685, Val Seq Acc=0.1653\n",
      "\n",
      "\n",
      "Epoch 84 Summary: Train Loss=0.8685\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85 Train Loss: 0.8701 | LR: 2.7e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.89it/s]\n",
      "Epoch 85 Evaluating:   1%|          | 1/124 [00:00<01:12,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 85) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2k|\\sin\\alpha\\tan\\alpha\\tan\\beta\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{m,n+i\\sin\\alpha\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\tan\\alpha_{n+1}\\alpha\\tan\\beta\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2\\alpha_{n+1}\\alpha\\tan\\beta\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : |\\sinz|\\sinw|\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85 Evaluating: 100%|██████████| 124/124 [04:47<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 85 Summary: Train Loss=0.8701, Val Seq Acc=0.1511\n",
      "\n",
      "\n",
      "Epoch 85 Summary: Train Loss=0.8701\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86 Train Loss: 0.8705 | LR: 2.7e-04: 100%|██████████| 1105/1105 [04:41<00:00,  3.93it/s]\n",
      "Epoch 86 Evaluating:   1%|          | 1/124 [00:02<04:13,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 86) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2k_{1+1}\\sin(18\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{1,n+1}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\pi_{n+1}m_{n}-18m\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 20\\lim\\limits_{n\\rightarrow\\infty}x_{n+18n}\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : 12\\lim\\limits_{n\\rightarrow\\infty}nx_{n+18}}\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86 Evaluating: 100%|██████████| 124/124 [02:35<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 86 Summary: Train Loss=0.8705, Val Seq Acc=0.1653\n",
      "\n",
      "\n",
      "Epoch 86 Summary: Train Loss=0.8705\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87 Train Loss: 0.8674 | LR: 2.7e-04: 100%|██████████| 1105/1105 [04:42<00:00,  3.91it/s]\n",
      "Epoch 87 Evaluating:   1%|          | 1/124 [00:02<05:10,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 87) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2k_{m-m-1}}\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{n_{m}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -a_{n+n}\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2\\alpha_{n+m}}\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : 12mn_{n+1}\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87 Evaluating: 100%|██████████| 124/124 [04:28<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 87 Summary: Train Loss=0.8674, Val Seq Acc=0.1785\n",
      "\n",
      "\n",
      "Epoch 87 Summary: Train Loss=0.8674\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88 Train Loss: 0.8671 | LR: 2.7e-04: 100%|██████████| 1105/1105 [04:41<00:00,  3.92it/s]\n",
      "Epoch 88 Evaluating:   1%|          | 1/124 [00:04<08:26,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 88) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2f_{n+1}\\tan\\alpha\\tan\\beta\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{1+1}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\frac{-T_{i}}\\sin\\alpha_{3}}}}}}{1+1-m-1}\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2\\frac{2\\tan\\alpha-\\beta}{1+\\tan^{2}\\alpha+\\beta}\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : |\\frac{2}+|b|}|}|}|}|}{|-|}+|}\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88 Evaluating: 100%|██████████| 124/124 [06:57<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 88 Summary: Train Loss=0.8671, Val Seq Acc=0.1785\n",
      "\n",
      "\n",
      "Epoch 88 Summary: Train Loss=0.8671\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89 Train Loss: 0.8672 | LR: 2.7e-04: 100%|██████████| 1105/1105 [04:42<00:00,  3.91it/s]\n",
      "Epoch 89 Evaluating:   1%|          | 1/124 [00:01<03:30,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 89) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2k_{1+man}=\\frac{1-n_{m}}a_{n}}{1-1)^{2}}\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : kM_{n+w}=\\sqrt{1}(w)z}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\frac{\\sqrt{2}+x_{4}\\sin\\alpha\\tan\\beta}{1+1+1}\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2\\alpha_{n+1}=\\frac{2\\cdot\\alpha_{n}18}{2n+1}\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : |\\frac{2}{1+y_{2}}|\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89 Evaluating: 100%|██████████| 124/124 [04:57<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 89 Summary: Train Loss=0.8672, Val Seq Acc=0.1430\n",
      "\n",
      "\n",
      "Epoch 89 Summary: Train Loss=0.8672\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90 Train Loss: 0.8648 | LR: 2.6e-04: 100%|██████████| 1105/1105 [04:43<00:00,  3.90it/s]\n",
      "Epoch 90 Evaluating:   1%|          | 1/124 [00:00<00:55,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 90) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2k_{n+n+n}\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{n+n+n}}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\tan\\alpha\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2\\tan\\alpha\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : 12\\tan\\alpha\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90 Evaluating: 100%|██████████| 124/124 [03:01<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90 Summary: Train Loss=0.8648, Val Seq Acc=0.1684\n",
      "\n",
      "\n",
      "Epoch 90 Summary: Train Loss=0.8648\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91 Train Loss: 0.8622 | LR: 2.6e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.89it/s]\n",
      "Epoch 91 Evaluating:   1%|          | 1/124 [00:01<02:41,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 91) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2k_{1+mn}}}}}}}}}}}}}}}}}}}}}}}\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{1n+mn}}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\sin_{n+m,n}}}}}}}}}}}}}\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2\\sin\\alpha_{n+mn}\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : |\\sinz|=\\sqrt{18+\\sinz}\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91 Evaluating: 100%|██████████| 124/124 [06:09<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 91 Summary: Train Loss=0.8622, Val Seq Acc=0.1755\n",
      "\n",
      "\n",
      "Epoch 91 Summary: Train Loss=0.8622\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92 Train Loss: 0.8602 | LR: 2.6e-04: 100%|██████████| 1105/1105 [04:43<00:00,  3.89it/s]\n",
      "Epoch 92 Evaluating:   1%|          | 1/124 [00:01<02:31,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 92) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2\\sqrt{n_{n}\\cosm-\\tann}\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{kn+1}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -T_{mn+mn}\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2\\sqrt{2n+mn}\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : |\\sum\\limits_{n\\rightarrow\\infty}nnnx|\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92 Evaluating: 100%|██████████| 124/124 [04:26<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 92 Summary: Train Loss=0.8602, Val Seq Acc=0.1957\n",
      "\n",
      "\n",
      "Epoch 92 Summary: Train Loss=0.8602\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93 Train Loss: 0.8592 | LR: 2.6e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.88it/s]\n",
      "Epoch 93 Evaluating:   1%|          | 1/124 [00:00<00:50,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 93) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2kmmnm\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : kM_{1+mn}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -T_{m+mn}\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2\\pi_{1+mn}\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : |\\sinw|\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93 Evaluating: 100%|██████████| 124/124 [02:14<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 93 Summary: Train Loss=0.8592, Val Seq Acc=0.1846\n",
      "\n",
      "\n",
      "Epoch 93 Summary: Train Loss=0.8592\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94 Train Loss: 0.8590 | LR: 2.6e-04: 100%|██████████| 1105/1105 [04:46<00:00,  3.86it/s]\n",
      "Epoch 94 Evaluating:   1%|          | 1/124 [00:00<00:48,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 94) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2\\tan\\alpha\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{mm+m)}}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\tan\\alpham\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2\\tan\\alpha\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : 12\\tan\\alpham\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94 Evaluating: 100%|██████████| 124/124 [02:41<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 94 Summary: Train Loss=0.8590, Val Seq Acc=0.2252\n",
      "\n",
      "\n",
      "==> Validation accuracy improved to 0.2252. Saving checkpoint for epoch 94...\n",
      "Чекпоинт сохранен в posformer_best_v4_lr_off.pth\n",
      "\n",
      "\n",
      "Epoch 94 Summary: Train Loss=0.8590\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95 Train Loss: 0.8583 | LR: 2.6e-04: 100%|██████████| 1105/1105 [04:44<00:00,  3.89it/s]\n",
      "Epoch 95 Evaluating:   1%|          | 1/124 [00:20<41:52, 20.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 95) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2\\sin\\alpha_{m+min}\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{m+min}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\sinw_{m+mm}}\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2\\sin\\alpha\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : |\\sinz|\\sinz\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95 Evaluating: 100%|██████████| 124/124 [05:40<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 95 Summary: Train Loss=0.8583, Val Seq Acc=0.1460\n",
      "\n",
      "\n",
      "Epoch 95 Summary: Train Loss=0.8583\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96 Train Loss: 0.8557 | LR: 2.6e-04: 100%|██████████| 1105/1105 [04:43<00:00,  3.89it/s]\n",
      "Epoch 96 Evaluating:   1%|          | 1/124 [00:00<01:16,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 96) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2\\sin\\alpha\\sin\\frac{\\sqrt{\\beta}}{1\\beta}\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{1,n+13m}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\sinx+\\siny\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2\\sin\\frac{2}\\alpha}{18}}\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : |\\sin\\frac{2}{\\sqrt{2\\pin+18n}}\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96 Evaluating: 100%|██████████| 124/124 [03:56<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 96 Summary: Train Loss=0.8557, Val Seq Acc=0.1846\n",
      "\n",
      "\n",
      "Epoch 96 Summary: Train Loss=0.8557\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97 Train Loss: 0.8555 | LR: 2.6e-04: 100%|██████████| 1105/1105 [04:42<00:00,  3.91it/s]\n",
      "Epoch 97 Evaluating:   1%|          | 1/124 [00:00<00:58,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 97) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2\\cdot\\sin\\alpha\\sin\\alpha\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{1+\\sin\\alpha\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\infty_{1+m+m}\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2\\sin\\alpha\\sin\\alpha\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : 12,\\sin\\alpha\\sin138\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97 Evaluating: 100%|██████████| 124/124 [03:28<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 97 Summary: Train Loss=0.8555, Val Seq Acc=0.1673\n",
      "\n",
      "\n",
      "Epoch 97 Summary: Train Loss=0.8555\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98 Train Loss: 0.8572 | LR: 2.6e-04: 100%|██████████| 1105/1105 [04:42<00:00,  3.91it/s]\n",
      "Epoch 98 Evaluating:   1%|          | 1/124 [00:00<01:44,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 98) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2k_{m,n}\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{m,n}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\lim\\limits_{n\\rightarrow\\infty}\\frac{\\sqrt{3}}{1111}}\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2\\sin\\alpha\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : |2x|\\sin\\frac{13}}{\\sqrt{2\\pin}}\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98 Evaluating: 100%|██████████| 124/124 [02:31<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 98 Summary: Train Loss=0.8572, Val Seq Acc=0.2150\n",
      "\n",
      "\n",
      "Epoch 98 Summary: Train Loss=0.8572\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99 Train Loss: 0.8539 | LR: 2.5e-04: 100%|██████████| 1105/1105 [04:42<00:00,  3.91it/s]\n",
      "Epoch 99 Evaluating:   1%|          | 1/124 [00:00<01:16,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 99) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2,\\sinw\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{m,n}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\frac{\\sqrt{1}-1}{m-mn}\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2a_{n+n}\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : |2\\cdot\\sin\\frac{\\sqrt{2}}{18mn}\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99 Evaluating: 100%|██████████| 124/124 [02:52<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 99 Summary: Train Loss=0.8539, Val Seq Acc=0.2211\n",
      "\n",
      "\n",
      "Epoch 99 Summary: Train Loss=0.8539\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100 Train Loss: 0.8528 | LR: 2.5e-04: 100%|██████████| 1105/1105 [04:42<00:00,  3.91it/s]\n",
      "Epoch 100 Evaluating:   1%|          | 1/124 [00:00<01:03,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 100) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2,\\sina+n+18\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{n+w}}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -x_{n+1}\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2x_{n+18}\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : 12x+\\sinx+13\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100 Evaluating: 100%|██████████| 124/124 [02:13<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100 Summary: Train Loss=0.8528, Val Seq Acc=0.2160\n",
      "\n",
      "\n",
      "Epoch 100 Summary: Train Loss=0.8528\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101 Train Loss: 0.8524 | LR: 2.5e-04: 100%|██████████| 1105/1105 [04:42<00:00,  3.91it/s]\n",
      "Epoch 101 Evaluating:   2%|▏         | 2/124 [00:00<00:37,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 101) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2_{\\sinw}\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{m+w}}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\lim\\limits_{n\\rightarrow\\infty}\\sin\\infty\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 2\\sinw\\sinz\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : 12\\sinw\\sinz\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101 Evaluating: 100%|██████████| 124/124 [01:57<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 101 Summary: Train Loss=0.8524, Val Seq Acc=0.2312\n",
      "\n",
      "\n",
      "==> Validation accuracy improved to 0.2312. Saving checkpoint for epoch 101...\n",
      "Чекпоинт сохранен в posformer_best_v4_lr_off.pth\n",
      "\n",
      "\n",
      "Epoch 101 Summary: Train Loss=0.8524\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102 Train Loss: 0.8500 | LR: 2.5e-04: 100%|██████████| 1105/1105 [04:43<00:00,  3.90it/s]\n",
      "Epoch 102 Evaluating:   1%|          | 1/124 [00:20<41:47, 20.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 102) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2k_{1,n+1}}\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{n_{w}}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\frac{1}{\\sqrt{2\\pin\\times9^{n}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 20i\\sinx\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : 12xmm\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102 Evaluating: 100%|██████████| 124/124 [03:05<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 102 Summary: Train Loss=0.8500, Val Seq Acc=0.2515\n",
      "\n",
      "\n",
      "==> Validation accuracy improved to 0.2515. Saving checkpoint for epoch 102...\n",
      "Чекпоинт сохранен в posformer_best_v4_lr_off.pth\n",
      "\n",
      "\n",
      "Epoch 102 Summary: Train Loss=0.8500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103 Train Loss: 0.8491 | LR: 2.5e-04: 100%|██████████| 1105/1105 [04:42<00:00,  3.91it/s]\n",
      "Epoch 103 Evaluating:   1%|          | 1/124 [00:01<02:53,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "====== EXAMPLES FROM VALIDATION (EPOCH 103) ======\n",
      "\n",
      "--- Example 1 ---\n",
      "  GROUND TRUTH: 2p\n",
      "  PREDICTED   : 2k=\\sqrt{x\\cos\\theta}\n",
      "\n",
      "--- Example 2 ---\n",
      "  GROUND TRUTH: kN\n",
      "  PREDICTED   : k_{1,-2n+13m}\n",
      "\n",
      "--- Example 3 ---\n",
      "  GROUND TRUTH: -7\n",
      "  PREDICTED   : -\\frac{\\sqrt{x+yww}(z)}}{m}}\n",
      "\n",
      "--- Example 4 ---\n",
      "  GROUND TRUTH: 20\n",
      "  PREDICTED   : 20,\\sin\\infty\n",
      "\n",
      "--- Example 5 ---\n",
      "  GROUND TRUTH: 12\n",
      "  PREDICTED   : 1,\\cos\\alpha\\sin\\frac{2}{13}\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103 Evaluating: 100%|██████████| 124/124 [02:52<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 103 Summary: Train Loss=0.8491, Val Seq Acc=0.2028\n",
      "\n",
      "\n",
      "Epoch 103 Summary: Train Loss=0.8491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 104 Train Loss: 0.8514 | LR: 2.5e-04:  25%|██▌       | 281/1105 [01:12<03:31,  3.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/1568963242.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# 5) Запуск обучения\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     train(\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36/558404377.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, val_dataset, epochs, batch_size, device, save_path)\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Хорошая практика\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m_no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0m_clip_grads_with_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m_no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m_get_total_norm\u001b[0;34m(tensors, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     total_norm = torch.linalg.vector_norm(\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_device\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnorms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "# Импортируем все необходимое\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Параметры\n",
    "    namphanvanhai_crohme_path = \"/kaggle/input\"\n",
    "    base_dir = os.path.join(namphanvanhai_crohme_path,\"CROHME\",\"train\")\n",
    "    caption_file = \"caption.txt\"\n",
    "    val_base_dir = os.path.join(namphanvanhai_crohme_path,\"CROHME\",\"2014\")\n",
    "    val_caption_file = \"caption.txt\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    SAVE_PATH = \"posformer_best_v4_lr_off.pth\"\n",
    "    VOCAB_PATH = \"crohme_vocab.pkl\"\n",
    "\n",
    "    print(\"Загрузка данных для словаря...\")\n",
    "    all_latex = []\n",
    "    with open(os.path.join(base_dir, caption_file), encoding='utf-8') as f:\n",
    "        for line in f: all_latex.append(line.strip().split('\\t', 1)[1])\n",
    "    with open(os.path.join(val_base_dir, val_caption_file), encoding='utf-8') as f:\n",
    "        for line in f: all_latex.append(line.strip().split('\\t', 1)[1])\n",
    "\n",
    "    print(\"Создание словарей...\")\n",
    "    vocab = Vocab(all_latex)\n",
    "    pos_vocab = PosVocab()\n",
    "\n",
    "    with open(VOCAB_PATH, \"wb\") as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    print(f\"Словарь сохранен в {VOCAB_PATH}, размер: {len(vocab.itos)} токенов.\")\n",
    "\n",
    "    print(\"Создание датасетов...\")\n",
    "    # Используем один и тот же `vocab` для обоих наборов\n",
    "    train_dataset = CROHMEDataset(base_dir, caption_file, vocab, pos_vocab, is_train=True)\n",
    "    val_dataset = CROHMEDataset(val_base_dir, val_caption_file, vocab, pos_vocab, is_train=False)\n",
    "    print(f\"Размер обучающего набора: {len(train_dataset)}, валидационного: {len(val_dataset)}\")\n",
    "\n",
    "    print(\"Инициализация модели...\")\n",
    "    model = PosFormer(main_vocab=vocab, pos_vocab=pos_vocab).to(device)\n",
    "\n",
    "    # 5) Запуск обучения\n",
    "    train(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        device=device,\n",
    "        save_path=SAVE_PATH\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-06T09:55:06.707033Z",
     "iopub.status.busy": "2025-07-06T09:55:06.706770Z",
     "iopub.status.idle": "2025-07-06T09:55:08.561196Z",
     "shell.execute_reply": "2025-07-06T09:55:08.560366Z",
     "shell.execute_reply.started": "2025-07-06T09:55:06.707015Z"
    },
    "id": "W1cZDIwrZJ6T",
    "outputId": "e979683b-3c0f-435e-e236-597b0917dfd7",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка словаря...\n",
      "Инициализация модели...\n",
      "Загрузка весов из /kaggle/working/best_model_onecyclelr_without_label_smoothing.pth...\n",
      "Веса успешно загружены.\n",
      "\n",
      "Начинаю распознавание...\n",
      "==============================\n",
      "Распознанная формула: f(x)=\\sqrt{x^{2}}\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_beam_search(model, image_tensor, beam_size, max_len, device):\n",
    "    \"\"\"\n",
    "    ИСПРАВЛЕННАЯ ВЕРСИЯ Beam Search для модели PosFormer.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.reset_iac_state()\n",
    "\n",
    "    # 1. Энкодер вызывается один раз\n",
    "    img_mask = torch.zeros_like(image_tensor[:, 0, :, :], dtype=torch.bool)\n",
    "    vis_feats, mem_pad_mask = model.encoder(image_tensor.unsqueeze(0), img_mask.unsqueeze(0))\n",
    "    B, Lk, D = vis_feats.shape\n",
    "    # Расширяем для beam search\n",
    "    vis_feats = vis_feats.expand(beam_size, Lk, D)\n",
    "    if mem_pad_mask is not None:\n",
    "        mem_pad_mask = mem_pad_mask.expand(beam_size, Lk)\n",
    "\n",
    "    # 2. Инициализация лучей\n",
    "    # Каждый луч: (последовательность, логарифмическая вероятность)\n",
    "    k_beams = [(torch.full((1, 1), model.sos_id, dtype=torch.long, device=device), 0.0)]\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        all_candidates = []\n",
    "        # Расширяем IAC состояние для каждого луча\n",
    "        for seq, score in k_beams:\n",
    "            if seq[0, -1] == model.eos_id:\n",
    "                all_candidates.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            # --- КОРРЕКТНЫЙ ВЫЗОВ ДЕКОДЕРА ---\n",
    "            # Готовим TGT из уже сгенерированных токенов (не из Position Forest)\n",
    "            tgt_embeds = model.emb_tok(seq) + model.dec_pos_enc[:, :seq.size(1), :]\n",
    "            tgt_mask = torch.triu(torch.ones(seq.size(1), seq.size(1), device=device), 1).bool()\n",
    "\n",
    "            # В инференсе с beam=1 мы бы передали (tgt_embeds, vis_feats[0], ...),\n",
    "            # но т.к. vis_feats уже расширен, мы можем просто передать его как есть.\n",
    "            # Мы декодируем по одному лучу за раз\n",
    "            dec_out = model.dec_norm(model._decode(tgt_embeds, vis_feats[0:1], seq, tgt_mask, None, mem_pad_mask[0:1] if mem_pad_mask is not None else None))\n",
    "\n",
    "            logits = model.head_tok(dec_out[:, -1, :])\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "            top_log_probs, top_ids = torch.topk(log_probs, beam_size, dim=-1)\n",
    "\n",
    "            for i in range(beam_size):\n",
    "                next_id = top_ids[0, i].unsqueeze(0).unsqueeze(0)\n",
    "                new_seq = torch.cat([seq, next_id], dim=1)\n",
    "                new_score = score + top_log_probs[0, i].item()\n",
    "                all_candidates.append((new_seq, new_score))\n",
    "\n",
    "        # Сортируем всех кандидатов и выбираем `beam_size` лучших\n",
    "        ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
    "        k_beams = ordered[:beam_size]\n",
    "\n",
    "        if all(b[0][0, -1] == model.eos_id for b in k_beams): break\n",
    "\n",
    "    # Нормализуем на длину и возвращаем лучший\n",
    "    best_beam = sorted(k_beams, key=lambda x: x[1] / len(x[0][0]), reverse=True)[0]\n",
    "    return best_beam[0].squeeze(0)\n",
    "\n",
    "\n",
    "def predict(model, image_path, vocab, beam_size, device):\n",
    "    \"\"\"Обертка для загрузки изображения и запуска предсказания.\"\"\"\n",
    "    # ИСПРАВЛЕНО: Трансформация должна соответствовать обучению (1 канал, нормализация 0.5)\n",
    "    resizer = ResizeWithPadding(IMG_HEIGHT, IMG_WIDTH, padding_value=255) # Белый фон для паддинга\n",
    "    img_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('L') # Конвертируем в grayscale\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Ошибка: Файл не найден {image_path}\")\n",
    "        return \"\"\n",
    "\n",
    "    img, _ = resizer(img)\n",
    "    image_tensor = img_transform(img).to(device)\n",
    "\n",
    "    predicted_ids = generate_beam_search(model, image_tensor, beam_size, MAX_SEQ_LEN, device)\n",
    "\n",
    "    # Декодирование\n",
    "    pred_list = predicted_ids.cpu().tolist()\n",
    "    try:\n",
    "        eos_index = pred_list.index(model.eos_id)\n",
    "        pred_list = pred_list[:eos_index]\n",
    "    except ValueError: pass\n",
    "\n",
    "    return vocab.decode(pred_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    CHECKPOINT_PATH = \"posformer_best.pth\"\n",
    "    VOCAB_PATH = \"crohme_vocab.pkl\"\n",
    "    IMAGE_TO_PREDICT = \"/path/to/your/image.png\"\n",
    "    BEAM_SIZE = 5\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(\"Загрузка словаря...\")\n",
    "    with open(VOCAB_PATH, 'rb') as f: vocab = pickle.load(f)\n",
    "    pos_vocab = PosVocab()\n",
    "\n",
    "    print(\"Инициализация модели...\")\n",
    "    # ИСПРАВЛЕНО: Используем правильный конструктор\n",
    "    model = PosFormer(main_vocab=vocab, pos_vocab=pos_vocab).to(device)\n",
    "\n",
    "    # Загружаем веса\n",
    "    load_checkpoint(model, None, None, CHECKPOINT_PATH, device)\n",
    "\n",
    "    if not os.path.exists(IMAGE_TO_PREDICT):\n",
    "        print(f\"ОШИБКА: Файл изображения не найден: {IMAGE_TO_PREDICT}\")\n",
    "    else:\n",
    "        print(\"\\nНачинаю распознавание...\")\n",
    "        predicted_latex = predict(model, IMAGE_TO_PREDICT, vocab, BEAM_SIZE, device)\n",
    "        print(\"=\"*30)\n",
    "        print(f\"Распознанная формула: {predicted_latex}\")\n",
    "        print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2OYdz1yM-pV",
    "outputId": "2335680f-85f0-4c06-a081-43cedc4a4c2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IYHweeW2UFhI",
    "outputId": "bd6703fa-e0a8-4f18-901e-f4bfcb676eb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficient Beam search result: [1, 22, 24, 81, 85, 29, 84, 94, 28, 59, 5, 21, 86, 113, 86, 42, 36, 86, 45, 5, 48, 91, 10, 113, 48, 74, 10, 42, 36, 113, 38, 5, 74, 42, 16, 99, 73, 113, 5, 74, 5, 70, 98, 84, 98, 24, 20, 72, 5, 86, 73, 42, 66, 84, 45, 4, 83, 42, 39, 113, 21, 22, 79, 86, 89, 86, 45, 67, 102, 74, 42, 113, 99, 5, 68, 73, 113, 56, 86, 80, 18, 86, 40, 71, 40, 77, 73, 16, 72, 14, 72, 5, 9, 25, 56, 67, 56, 59, 58, 58]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5746619,
     "sourceId": 9453794,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
